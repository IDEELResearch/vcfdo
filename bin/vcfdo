#! /usr/bin/env python3
"""
vcfdo
Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.
"""

import os
import sys
import numpy as np
import itertools as it
import argparse
import logging

np.warnings.filterwarnings("ignore")

import pybedtools as pbt
from itertools import permutations, combinations
from re import split as resplit
from datetime import datetime as dt
from collections import defaultdict, OrderedDict
from cyvcf2 import VCF
from allel import pca, randomized_pca, pairwise_distance, locate_unlinked
#from allel.stats import rogers_huff_r
from scipy.spatial.distance import squareform
from scipy.optimize import minimize as minimize_fn, minimize_scalar
from pyfaidx import Fasta

from vcfdo import core

def block_jackknife(values, statistic, block_size = 1):

	## how long is input? allow to be multidimensional, assume replicates are in rows
	#print("values have shape {}".format(values.shape))
	if values.ndim < 2:
		values = np.atleast_2d(values).T
	n = values.shape[0]

	## how many blocks, respecting (approx) the block size?
	m = block_size
	M = int(n/m)+1

	## split into a list of arrays, each item a block
	blocks = np.array_split(np.indices((n,))[0], M)
	blocks = [ _ for _ in blocks if _.shape[0] > 0 ]
	M = len(blocks)

	## not enough blocks to get standard errors? too bad
	if M < 3:
		se = np.nan
	else:

		## loop on blocks
		for jj, delete_j in enumerate(blocks):
			## use mask out this block
			idx = np.tile(True, n)
			#print("To delete = {}".format(delete_j))
			idx[delete_j] = False
			keep = values[ idx,... ]
			#print("Keepers have shape {}".format(keep.shape))
			## calculate statistic on remaining values
			theta_j = statistic(keep)
			if jj == 0:
				theta = np.zeros((M, theta_j.shape[0]))
				#print("thetas have shape {}".format(theta.shape))
			#print("theta_j = {}".format(theta_j))
			theta[jj] = theta_j

		## calculate standard error
		theta_bar = np.mean(theta, 0)
		#print("theta_bar = {}".format(theta_bar))
		se = np.sqrt( ((M-1)/M) * np.sum(np.power(theta - theta_bar, 2), 0) )

	#print("se = {}".format(se[...]))

	## return se
	return se, None

def calc_wsaf(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	vcf.add_info_to_header({ "ID": "PLAF", "Description": "Population-level ALT allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "PLMAF", "Description": "Population-level minor allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "UNW", "Description": "Population-level evidence for minor allele, as if all reads were pooled", "Number": 1, "Type": "Integer" })
	vcf.add_format_to_header({ "ID": "WSAF", "Description": "Within-sample allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_format_to_header({ "ID": "WSMAF", "Description": "Within-sample MAJOR allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })

	do_filter = args.set_filter is not None
	if do_filter:
		filter_at = min(1 - args.set_filter, args.set_filter)
		vcf.add_filter_to_header({ "ID": "PseudoHet", "Description": "WSAF out-of-bounds for a pure isolate" })
		vcf.add_format_to_header({ "ID": "FT", "Description": "Genotype-level filter", "Number": ".", "Type": "String" })

	vcf.write_header()

	## loop on VCF and add 'WSAF','PLAF','PLMAF' tags
	vcf.add_standard_counter("multiallelic")
	vcf.add_standard_counter("nonpolymorphic")
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		dp = site.format("AD")
		dp[ dp < 0 ] = 0
		## is depth recorded for >1 allele?
		if dp.shape[1] >= 2:
			# > 1 allele counted
			dp_ref = dp[:,0]
			dp_alt = np.sum(dp[:,1:], 1)
			dp_maj = np.amax(dp, 1)
			dp_tot = np.sum(dp, 1)

			## are there actually any reads for the non-ref alleles?
			dp_tot_all = np.sum(dp)
			dp_alt_all = np.sum(dp[:,1:])
			dp_ref_all = np.sum(dp[:,0])

			wsmaf = dp_maj / dp_tot
			wsaf = dp_alt / (dp_ref + dp_alt)
			# if requested, mask sites where no genotype call was made (for any reason)
			if args.ignore_missing:
				wsaf[ site.gt_types == 3 ] = np.nan
				wsmaf[ site.gt_types == 3 ] = np.nan

			# if requested, mask sites where depth is below threshold
			if args.min_depth > 0:
				wsaf[ dp_tot < args.min_depth ] = np.nan
				wsmaf[ dp_tot < args.min_depth ] = np.nan

			# if requested, mask sites where GQ is below threshold
			if args.min_GQ:
				gqs = site.format("GQ")[:,0]
				wsaf[ gqs < args.min_GQ ] = np.nan
				wsmaf[ gqs < args.min_GQ ] = np.nan

			# convert to masked array to allow calclation of PLMAF
			wsaf = np.ma.masked_invalid(wsaf)

			## figure out which allele is minor in population
			## NB: for UNW, record number of reads supporting minor allele -- which may be MORE than major allele,
			##  depending on distribution per-sample coverage. But we want evidence of favor of rarest allele.
			plaf = np.average(wsaf)
			plmaf = -1
			if plaf < 0.5:
				plmaf = plaf
				unw = dp_alt_all
			else:
				plmaf = 1 - plaf
				unw = dp_ref_all
			site.INFO["PLAF"] = float(plaf)
			site.INFO["PLMAF"] = float(plmaf)
			site.INFO["UNW"] = int(unw)

			# now undo masking of NaNs
			wsaf[ np.isnan(wsaf) ] = -1
			wsmaf[ np.isnan(wsmaf) ] = -1
			site.set_format("WSAF", wsaf)
			site.set_format("WSMAF", wsmaf)

		else:
			# only 1 allele counted
			site.INFO["PLAF"] = 0.0
			site.INFO["PLMAF"] = 0.0
			site.set_format("WSAF", np.tile(0.0, len(vcf.samples)))

		if do_filter:
			gt_filt = np.tile("PASS", len(vcf.samples))
			gt_filt[ np.logical_or(wsaf > args.set_filter, wsaf < (1-args.set_filter)) ] = "PseudoHet"
			if site.has_format("FT"):
				old_filt = site.get_format("FT")
				for ii, flt in enumerate(old_filt):
					if flt != "PASS":
						gt_filt[ii] = flt + ";" + gt_filt[ii]
			site.set_format("FT", gt_filt)

		site.write_out()

	vcf.show_count_summary()


def calc_wsaf_hist(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	## set number of bins in histogram
	nbins = int(1/args.binwidth)+1
	bins = np.linspace(0,1,nbins)
	vcf.log.info("Histogram bin width = {}".format(args.binwidth))
	vcf.log.info("Histogram bin edges = {}".format(bins))

	## initialize histogram
	counts = np.zeros( (nbins, vcf.nsamples), dtype = np.int )

	## loop on VCF and gather WSAF etc
	vcf.add_counter("nused","sites had usable data")
	vcf.add_standard_counter("multiallelic")
	vcf.add_standard_counter("nowsaf")
	for site in vcf:

		if site.is_multiallelic or (not site.has_wsaf):
			continue

		## get (masked) WSAFs and decide which bin they fall on
		wsaf = site.get_wsaf()
		wsaf[ wsaf > 0.999999 ] = 0.999999 # hack to handle boundary at WSAF == 1 since bins treated as right-open
		to_bin = np.digitize(wsaf, bins)
		#vcf.log.debug(to_bin)

		## add 0 to count for missing WSAF, 1 otherwise
		flag = (~wsaf.mask).astype(np.int)

		## construct complicated slice-indexer
		iidx = tuple( range(0, counts.shape[1]) )
		binidx = tuple( to_bin )

		## increment counts
		counts[ binidx,iidx ] += flag

		vcf.counters["nused"].increment()

	vcf.show_count_summary()

	vcf.log.info("Printing histograms ...")
	for ii, iid in enumerate(vcf.samples):
		for jj in range(0, counts.shape[0]):
			print(iid, "{:0.03f}".format(bins[jj]), counts[jj,ii], sep = "\t")

	## all done
	vcf.log.info("Done.\n")

def calc_fws(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	## loop on VCF and gather WSAF etc
	vcf.add_counter("nused","sites had usable data")
	vcf.add_standard_counter("multiallelic")
	vcf.add_standard_counter("nowsaf")
	vcf.add_standard_counter("nonpolymorphic")
	vcf.log.info("Processing VCF file ...")
	H_s = []
	H_p = []

	for site in vcf:

		if site.is_multiallelic or (not site.has_wsaf) or (not site.is_polymorphic):
			continue

		wsaf = site.get_wsaf()
		plmaf = min(np.average(wsaf), 1 - np.average(wsaf))
		#plmaf = site.INFO["PLMAF"]
		H_px = 1 - (plmaf**2 + (1 - plmaf)**2)
		H_sx = 1 - (np.power(wsaf, 2) + np.power((1 - wsaf), 2))
		H_p.append(H_px)
		H_s.append(H_sx)

	vcf.counters["nused"].increment( len(H_p) )
	vcf.show_count_summary()

	vcf.log.info("Calculating F_ws ...")
	if args.jackknife is not None:
		vcf.log.info(" [ standard errors estimated by block jackknife in blocks of size ~= {} ]".format(args.jackknife))
	else:
		vcf.log.info(" [ asymptotic standard errors ]")
	## matrices of H_px and H_sx
	H_p = np.array(H_p, dtype = np.float).reshape(len(H_p), 1)
	H_s = np.ma.asanyarray(H_s)
	#print(np.sum(H_s.mask))
	#print(H_s.shape)

	def do_fws(Z, do_se = False):
		y = Z[:,0,None] # None stops numpy dropping a dimension here
		X = Z[:,1:]
		beta, sse, rank, sv = np.linalg.lstsq(y, X, rcond = -1) # `rcond` passed explicitly to avoid FutureWarning
		if do_se:
			sigma2 = np.sum((y - np.dot(X, beta))**2) / (X.shape[0] - 1)
			C = sigma2 * np.linalg.pinv(np.dot(X.T, X)) # covariance matrix
			se = np.sqrt(np.diag(C)) # standard error
		F_ws = 1 - beta
		if do_se:
			return F_ws[0,:], se
		else:
			return F_ws[0,:]

	## do no-intercept regression of H_sx on H_px
	for ii,iid in enumerate(vcf.samples):
		H_sx = H_s[:,ii]
		X = H_sx[ ~H_sx.mask ]
		X = X.reshape(len(X), 1)
		y = H_p[ ~H_sx.mask ]
		Z = np.hstack((y,X))
		if Z.shape[0] > 0:
			if args.jackknife is not None:
				se, _ = block_jackknife(Z, do_fws, args.jackknife)
				F_ws = do_fws(Z)
				if np.isnan(se):
					se = np.tile(np.nan,1)
			else:
				F_ws, se = do_fws(Z, True)
			print(iid, np.asscalar(F_ws), np.asscalar(se), np.sum(~H_sx.mask), sep = "\t")
		else:
			print(iid, np.nan, np.nan, 0, sep = "\t")

	vcf.log.info("Done.\n")


def pca_wsaf(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## decide which samples to keep
	keep_samples = vcf.reconcile_samples(args.samples, args.samples_file, allow_all = True)
	sample_idx = [ vcf.samples.index(_) for _ in keep_samples ]

	## are we projecting some samples onto others?
	if args.project is not None:
		target_samples = vcf.reconcile_samples([], args.project, allow_all = True)
		target_idx = [ vcf.samples.index(_) for _ in target_samples ]
		query_samples = set(keep_samples) - set(target_samples)
		query_idx = [ vcf.samples.index(_) for _ in query_samples ]
		sample_idx = query_idx + target_idx
		if not len(target_idx):
			raise ValueError("Must provide valid set of 'target' samples to define PC space.")
		elif not len(query_idx):
			# bail out and just do PCA as normal
			vcf.log.warning("No valid query sampels provided; proceeding with usual one-pass PCA.")
			args.project = None
		else:
			vcf.log.info("Preparing to do two-pass PCA with {} target and {} query samples ...".format(len(target_idx), len(query_idx)))
	else:
		vcf.log.info("Preparing to do one-pass PCA on {} samples ..".format(len(sample_idx)))

	W = vcf.add_chunk(None, args.chunk_size, nsamples = len(sample_idx))
	vcf.log.info("Reading genotype data in chunks of size {} ...".format(args.chunk_size))
	vcf.add_standard_counter("multiallelic")
	vcf.add_counter("nmissing","sites with WSAF missingness > {}".format(args.max_missing))
	vcf.add_counter("nrare","sites with PLMAF < {}".format(args.min_plmaf))
	nkept = 0
	for site in vcf:

		if site.is_multiallelic:
			continue

		w_unmasked = site.get_wsaf(sample_idx)
		if np.sum(w_unmasked.mask)/len(sample_idx) > args.max_missing:
			vcf.counters["nmissing"].increment()
			continue

		plmaf = site.recalc_plmaf(sample_idx)
		if plmaf < args.min_plmaf:
			vcf.counters["nrare"].increment()
			continue

		nkept += 1
		if nkept >= W.shape[0]:
			vcf.log.debug("Increasing array allocation ... ")
			W = vcf.add_chunk(W, args.chunk_size)

		## get WSAFs from FORMAT field, imputing PLAFs for missing
		w, plaf = site.get_imputed_wsaf(sample_idx)
		W[ nkept,: ] = w

	vcf.show_count_summary()

	## remove low-variance sites that make bad things happen in SVD
	W = W[:nkept,:]
	if args.project is not None:
		# when projecting, the 'target' samples define PC space, so calculate row variance only in that subset
		w_target_cols = list(range(0, len(target_idx)))
		w_query_cols = [ len(query_idx) + _ for _ in range(0, len(query_idx)) ]
		sigma = np.var(W[ :,w_target_cols ], 1)
	else:
		sigma = np.var(W, 1)
	W = W[ (sigma > 1.0e-6),:]
	vcf.log.info("Loaded genotypes at {} biallelic sites with PLMAF >= {}".format(nkept, args.min_plmaf))
	vcf.log.info("Retained {} sites with non-negligible variance for PCA.".format(W.shape[0]))

	## do the PCA using scikit-allel tools
	## are we projecting some samples onto others?
	if args.project is not None:
		vcf.log.info("Performing PCA with target samples to define PC space ...")
		pca_fn = pca if not args.randomized else randomized_pca
		_, model = pca_fn(W[ :,w_target_cols ], n_components = args.ncomponents, scaler = "patterson")
		vcf.log.info("Done; now projecting remaining samples onto those PCs ...")
		proj_coords = model.transform(W)
		vcf.log.info("Done with projection.")
	else:
		vcf.log.info("Performing PCA  ...")
		pca_fn = pca if not args.randomized else randomized_pca
		proj_coords, model = pca_fn(W, n_components = args.ncomponents, scaler = "patterson")
		vcf.log.info("Done with PCA.")

	## write rotations and normalized eigenvalues
	vcf.log.info("Writing results to <{0}.pca> and <{0}.pca.ev> (sample coordinates and variance explained, respectively.)".format(args.out))
	with open(args.out + ".pca", "w") as pcs_file:
		for ii in range(0, proj_coords.shape[0]):
			print(vcf.samples[ sample_idx[ii] ], *proj_coords[ii,:], sep = "\t", file = pcs_file)
	with open(args.out + ".pca.ev", "w") as evs_file:
		print("\n".join([ str(_) for _ in model.explained_variance_ratio_ ]), file = evs_file)

	## write loadings, if requested
	#if args.write_loadings:

	## all done
	vcf.log.info("Done.\n")

def calc_weighted_dist(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	wsaf = vcf.add_chunk(None, args.chunk_size)
	wsaf_imputed = vcf.add_chunk(None, args.chunk_size)

	## loop on VCF and add take sample depths
	vcf.log.info("Reading genotype data in chunks of size {} ...".format(args.chunk_size))
	for site in vcf:

		if vcf.ii >= wsaf.shape[0]:
			vcf.log.debug("Increasing array allocation ... ")
			wsaf = vcf.add_chunk(wsaf, args.chunk_size)
			wsaf_imputed = vcf.add_chunk(wsaf_imputed, args.chunk_size)

		## get WSAFs from FORMAT field
		this_wsaf = site.get_wsaf()
		wsaf[ vcf.ii-1,: ] = this_wsaf

		## impute PLAF for missing values
		w, plaf = site.get_imputed_wsaf()
		wsaf_imputed[ vcf.ii-1,: ] = w

	vcf.show_count_summary()

	wsaf = wsaf[ :vcf.ii, : ]
	wsaf_imputed = wsaf_imputed[ :vcf.ii, : ]
	vcf.log.info("Dimensions of full genotype matrix: {}".format(wsaf.shape))

	if not args.unweighted:
		vcf.log.info("Calculating weighting factors following Amato et al (PMC4786412) ...")
		r_sq = np.corrcoef(wsaf_imputed) ** 2
		vcf.log.debug("r_sq {}".format(r_sq.shape))
		r_sq[ np.isnan(r_sq) ] = 0
		r_sq -= np.eye(r_sq.shape[0])
		r_sq[ r_sq < kwargs["rsq_cutoff"] ] = 0
		w_i = 1 / (1 + np.sum(r_sq, 1))
		vcf.log.debug("r_sq = {}".format(np.round(100*r_sq)))
		vcf.log.debug("w_i = {}".format(w_i.T))
		vcf.log.debug("variant weights in range [{}, {}]".format(np.min(w_i), np.max(w_i)))
	else:
		vcf.log.info("Calculating unweighted distance, ie. ignoring effects of LD between sites ...")
		w_i = np.ones(wsaf_imputed.shape[0])

	vcf.log.info("Calculating pairwise distances using WSAFs ...")
	combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
	vcf.log.info("\t[{} combinations]".format(len(combos)))
	formatter = "{:0." + str(args.digits) + "f}"
	#D = np.zeros( (len(vcf.samples), len(vcf.samples)), dtype = np.float )
	ii = 0
	for sa,sb in combos:
		ii += 1
		fa = np.ma.masked_values(wsaf[:,sa], -1)
		fb = np.ma.masked_values(wsaf[:,sb], -1)
		d_u = (fa*(1-fb) + (1-fa)*fb)
		d_i = d_u*w_i
		d = np.average(d_i)
		du = np.average(d_u)
		#D[ sa,sb ] = np.average(d*w_i)
		nz = np.sum(~d_i.mask)
		if not ii % 100:
			vcf.log.info("\t... {} pairs done ...".format(ii))
		print(vcf.samples[sa], vcf.samples[sb], nz, formatter.format(d), formatter.format(du), sep = "\t")

	vcf.log.info("Done.\n")


def calc_malecot_ibd(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	wsaf = vcf.add_chunk(None, args.chunk_size)
	wsaf_imputed = vcf.add_chunk(None, args.chunk_size)
	plafs = vcf.add_chunk(None, args.chunk_size, nsamples = 1)

	## loop on VCF and add take sample depths
	vcf.log.info("Reading genotype data in chunks of size {} ...".format(args.chunk_size))
	vcf.log.debug("PLAF shape = {}, geno shape = {}".format(plafs.shape, wsaf.shape))
	for site in vcf:

		if vcf.ii >= wsaf.shape[0]:
			vcf.log.debug("Increasing array allocation ... ")
			wsaf = vcf.add_chunk(wsaf, args.chunk_size)
			wsaf_imputed = vcf.add_chunk(wsaf_imputed, args.chunk_size)
			plafs = vcf.add_chunk(plafs, args.chunk_size)

		## get WSAFs from FORMAT field
		this_wsaf = site.get_wsaf()
		wsaf[ vcf.ii-1,: ] = this_wsaf

		## impute PLAF for missing values
		w, plaf = site.get_imputed_wsaf()
		#vcf.log.debug("ii={} {}@{} INFO/PLAF={}, calc PLAF={}".format(vcf.ii-1, site.CHROM, site.POS, site.INFO["PLAF"], plaf))
		plafs[ vcf.ii-1,0 ] = plaf
		wsaf_imputed[ vcf.ii-1,: ] = w

	vcf.show_count_summary()

	## trim off unused allocation
	wsaf = wsaf[ :vcf.ii, : ]
	wsaf_imputed = wsaf_imputed[ :vcf.ii, : ]
	plafs = plafs[ :vcf.ii, 0 ]

	vcf.log.info("Dimensions of full genotype matrix: {}".format(wsaf.shape))
	vcf.log.info("Dimensions of PLAFs matrix: {}".format(plafs.shape))

	if args.self_only:
		vcf.log.info("Estimating individual inbreeding coefficients using WSAFs ...")
		combos = [ (_,_) for _ in range(0, len(vcf.samples)) ]
	else:
		vcf.log.info("Estimating pairwise IBD using WSAFs ...")
		combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
	vcf.log.info("\t[{} combinations]".format(len(combos)))
	vcf.log.info("eps = {}, maxiter = {}".format(args.tol, args.max_iter))
	formatter = "{:0." + str(args.digits) + "f}"
	#D = np.zeros( (len(vcf.samples), len(vcf.samples)), dtype = np.float )
	ii = 0

	def ibd_negloglik(f, wa, wb, plaf, inbreeding = True):
		"""
		Log-likelihood for Malecot (1970) definition of IBD
		Let plaf = population allele freq (non-ref allele), f = inbreeding coefficient.
		There are four cases (haplotypes match or don't match for major or minor allele).
		At each locus, we weight each of these according to WSAFs (wa, wb).
		Then take Log-likelihood across all sites.
		"""

		## four cases at each locus (corresponding to four cells in Punnet-like square)
		c1 = f*plaf + (1-f)*(plaf**2)
		c2 = (1-f)*plaf*(1-plaf)
		c3 = c2
		c4 = f*(1-plaf) + (1-f)*((1-plaf)**2)

		## define these for notational convenience
		pa = wa
		qa = 1-wa
		pb = wb
		qb = 1-wb

		## likelihood is weighted sum of these cases; here we do some logging and unlogging for underflow protection
		if not inbreeding:
			lik =	pa*pb*c1 + \
					pa*qb*c2 + \
					qa*pb*c3 + \
					qa*qb*c4
		else:
			lik =	(pa == 1.0)*c1 + \
					(pa == 0.5)*c2 + \
					(pa == 0.5)*c3 + \
					(pa == 0.0)*c4
		loglik = -1*np.sum(np.log(lik))
		return loglik

	for sa,sb in combos:
		ii += 1
		wa = np.ma.masked_values(wsaf[:,sa], -1)
		wb = np.ma.masked_values(wsaf[:,sb], -1)
		keepers = np.logical_and(~np.ma.getmaskarray(wa), ~np.ma.getmaskarray(wb))
		nz = np.sum(keepers)
		#rez = minimize_fn(ibd_negloglik, 0.1, (wa,wb,plafs), bounds = [ (0,1) ], method = "Nelder-Mead")
		if sa == sb:
			eps = args.wsaf_threshold
			wa[ np.logical_and(wa > eps, wa < 1-eps) ] = 0.5
			wa[ wa <= eps ] = 0.0
			wa[ wa >= 1-eps ] = 1.0
			wb = wa
			vcf.log.debug("{} 0,1,2 = ({})".format(vcf.samples[sa], (np.sum(wa == 0.0), np.sum(wa == 0.5), np.sum(wa == 1.0))))
		rez = minimize_scalar(ibd_negloglik, bounds = (0.001,0.999), \
							args = (wa[keepers],wb[keepers],plafs[keepers], args.self_only), \
							method = "Bounded", options = { "maxiter": args.max_iter, "xatol": args.tol })
		if rez.success:
			fhat = rez.x
		else:
			fhat = np.nan
		if not ii % 100:
			vcf.log.info("\t... {} pairs done ...".format(ii))
		print(vcf.samples[sa], vcf.samples[sb], nz, formatter.format(fhat), sep = "\t")

	vcf.log.info("Done.\n")


def calc_ibs_dist(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	geno = vcf.add_chunk(None, args.chunk_size, np.float)
	geno_imputed = vcf.add_chunk(None, args.chunk_size, np.float)

	## loop on VCF and add take sample depths
	nused = 0
	vcf.add_counter("nnonpoly","sites not polymorphic in called genotypes")
	vcf.log.info("Reading genotype data in chunks of size {} ...".format(args.chunk_size))
	for site in vcf:

		this_geno = site.to_n_alt()/site.ploidy
		#vcf.log.debug("{}@{} ploidy = {}".format(site.CHROM, site.POS, site.ploidy))
		ac = np.sum(this_geno)
		an = np.sum(~this_geno.mask)
		if ac == an or ac == 0:
			vcf.counters["nnonpoly"].increment()
			continue
		nused += 1

		if nused >= geno.shape[0]:
			vcf.log.debug("Increasing array allocation ... ")
			geno = vcf.add_chunk(geno, args.chunk_size)
			geno_imputed = vcf.add_chunk(geno_imputed, args.chunk_size)

		## get genotype calls as number of alt alleles
		geno[nused,:] = this_geno[:]

		## impute PLAF for missing values
		plaf = np.average(this_geno)
		ww = this_geno[:]
		ww[ ww.mask ] = plaf
		geno_imputed[nused,:] = ww

	vcf.add_counter("nused","sites had usable data", nused)
	vcf.show_count_summary()

	geno = geno[ :nused, : ]
	geno_imputed = geno_imputed[ :nused, : ]
	vcf.log.info("Dimensions of full genotype matrix: {}".format(geno_imputed.shape))

	formatter = "{:0." + str(args.digits) + "f}"
	vcf.log.info("Calculating pairwise distances ...")
	#vcf.log.info("[ Assuming ploidy = {} for Hamming distances ... ]".format(vcf.ploidy))
	if args.flatten:
		D = np.eye(vcf.nsamples)
		nz = np.zeros( (vcf.nsamples,vcf.nsamples), dtype = np.int)
		combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
		vcf.log.info("\t[{} combinations]".format(len(combos)))
		ii = 0
		for sa,sb in combos:
			ii += 1
			fa = np.ma.masked_values(geno[:,sa], 3)
			fb = np.ma.masked_values(geno[:,sb], 3)
			#print(fa.shape, fb.shape)
			d = 1 - np.average(np.abs(fa-fb))
			if d is np.ma.masked:
				d = np.nan
			D[sa,sb] = d
			D[sb,sa] = d
			nz[sa,sb] = np.sum(np.logical_and(~fa.mask, ~fb.mask))
			nz[sb,sa] = nz[sa,sb]
			print(vcf.samples[sa], vcf.samples[sb], nz[sa,sb], formatter.format(d), sep = "\t")
			if not ii % 100:
				vcf.log.info("\t... {} pairs done ...".format(ii))

	else:

		D = squareform( pairwise_distance(geno_imputed, "hamming") )
		D = 1 - D
		print(".", *vcf.samples, sep = "\t")
		for jj,iid in enumerate(vcf.samples):
			rounded = [ formatter.format(_) for _ in D[jj,:] ]
			print(vcf.samples[jj], *rounded, sep = "\t")

	vcf.log.info("Done.\n")

def summarise_dp(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)

	sample_dp = vcf.add_chunk(None, args.chunk_size, np.int)

	## loop on VCF and add take sample depths
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		if vcf.ii >= sample_dp.shape[0]:
			vcf.log.debug("Increasing array allocation ... ")
			sample_dp = vcf.add_chunk(sample_dp, args.chunk_size)

		this_dp = site.format("DP").T[0:]
		this_dp[ this_dp < 0 ] = 0
		sample_dp[vcf.ii-1,:] = this_dp

	mu = np.mean(sample_dp[:vcf.ii,:], 0)
	sigma = np.std(sample_dp[:vcf.ii,:], 0)
	med = np.percentile(sample_dp[:vcf.ii,:], 50, 0)
	hi = np.percentile(sample_dp[:vcf.ii,:], 99, 0)

	for iid, _mu, _med, _sigma, _hi in zip(vcf.samples, mu, med, sigma, hi):
		print(iid, "{:.1f}".format(_mu), "{:.1f}".format(_med), "{:.3f}".format(_sigma), "{:.1f}".format(_hi), sep = "\t")

	vcf.show_count_summary()

def summarise_flt(vcf, **kwargs):

	## loop on VCF and add take sample depths
	what_mode = None
	if args.samples:
		counter = (
			{ iid: defaultdict(int) for iid in vcf.samples },
			{ iid: defaultdict(int) for iid in vcf.samples }
		)
		what_mode = "SAMPLES"
	elif args.sites:
		counter = defaultdict(int)
		what_mode = "SITES"
	else:
		vcf.log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	vcf.log.info("Talling filters by {}".format(what_mode))
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		site_filt = "PASS" if site.FILTER is None else site.FILTER
		if what_mode == "SITES":
			counter[site_filt] += 1
		elif what_mode == "SAMPLES":
			jj = 0 if site_filt == "PASS" else 1
			if "FT" in site.FORMAT:
				for iid, geno_filt in zip(vcf.samples, site.format("FT")):
					counter[jj][iid][geno_filt] += 1
			else:
				for iid in vcf.samples:
					counter[jj][iid]["PASS"] += 1

	#print(counter)
	if what_mode == "SITES":

		for filt,count in counter.items():
			print(filt, count, sep = "\t")

	elif what_mode == "SAMPLES":

		for iid in vcf.samples:
			for jj in (0,1):
				flag = "PASS" if ii else "FAIL"
				for filt,count in counter[jj][iid].items():
					print(iid, flag, filt, count, sep = "\t")

	vcf.show_count_summary()

def calc_missingness(vcf, **kwargs):

	## loop on VCF and count missing calls
	what_mode = None
	if args.samples:
		counts = np.zeros(len(vcf.samples), dtype = np.int)
		what_mode = "SAMPLES"
	elif args.sites:
		what_mode = "SITES"
	else:
		vcf.log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	vcf.log.info("Talling missingness by {}".format(what_mode))
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		if what_mode == "SITES":
			nmiss = np.sum(site.gt_types == 3)
			ntot = len(vcf.samples)
			print(site.CHROM, site.POS, site.ID, site.REF, ",".join(site.ALT), nmiss, nmiss/ntot, sep = "\t")
		elif what_mode == "SAMPLES":
			counts += (site.gt_types == 3)

	#print(counter)
	if what_mode == "SITES":
		pass
	elif what_mode == "SAMPLES":
		for iid,nmiss in zip(vcf.samples, counts):
			print(iid, nmiss, nmiss/ii, sep = "\t")

	vcf.show_count_summary()

def polarize_anc_vs_der(vcf, **kwargs):

	## connect to ancestral genome file
	genome = Fasta(args.fasta)
	vcf.log.info("Ancestral genome: <{}>".format(args.fasta))

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": "AA", "Description": "Ancestral allele inferred from outgroup", "Number": 1, "Type": "String" })
	vcf.write_header()

	def define_anc_allele(alleles, og):
		ref = alleles[0].upper()
		alt = [ x.upper() for x in alleles[1:] ]
		new_alleles = [ref] + alt
		og = og.upper()
		anc = "X"*len(ref)
		for ii,a in enumerate(new_alleles):
			if a == og:
				anc = alleles[ii]
				break
		return anc

	## loop on VCF and add 'AA' tag
	vcf.add_counter("nmiss","sites have undefined ancestral allele")
	vcf.add_counter("nanc","sites have REF == ancestral")
	vcf.add_counter("nder","sites have REF == derived")
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		start = site.POS - 1
		end = start + len(site.REF)
		alleles = [site.REF] + site.ALT
		og = str(genome[site.CHROM][start:end])
		anc = define_anc_allele(alleles, og)
		#print(og, alleles, anc, file = sys.stderr)

		if anc == "X":
			vcf.counters["nmiss"].increment()
		elif anc == alleles[0]:
			vcf.counters["nanc"].increment()
		elif anc in alleles[1:]:
			vcf.counters["nder"].increment()

		site.INFO["AA"] = anc
		site.write_out()

	vcf.show_count_summary()

def transition_or_transversion(vcf, **kwargs):

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": "Transversion", "Description": "Is this site a transition (0), transversion (1), or undefined (-1)?", "Number": 1, "Type": "Integer" })
	vcf.add_info_to_header({ "ID": "StrongWeak", "Description": "H-bond strength of ancestral and derived states (w=weak, s=strong)", "Number": 1, "Type": "String" })
	vcf.add_info_to_header({ "ID": "BGC", "Description": "Is this site a candidate for GC-biased gene coversion? 0=No, 1=Yes, -1=undefined", "Number": 1, "Type": "Integer" })
	vcf.write_header()

	def ti_or_tv(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		if subst in ["A>G","C>T","G>A","T>C"]:
			return 0
		else:
			return 1

	def is_bgc(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		if anc in ["A","T"] and der in ["G","C"]:
			return 1
		else:
			return 0

	def strong_or_weak(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		strength = { "A": "w", "C": "s", "G": "s", "T": "w" }
		return strength[anc] + strength[der]

	def good_alleles(alleles):
		alleles = "".join(alleles).upper()
		return all(_ in "ACGT" for _ in alleles)

	## loop on VCF
	nti, ntv, nbgc = 0, 0, 0
	nnotsnv = 0
	nnoanc = 0
	#nbadprefix = 0
	nbadallele = 0
	nmulti = 0
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		if not (site.is_snp and not site.is_indel):
			nnotsnv += 1
			continue

		if len(site.ALT) > 1:
			nmulti += 1
			continue

		if not good_alleles([site.REF] + site.ALT):
			nbadallele += 1
			continue

		## absent or bogus ancestral allele?
		if site.has_ancestral:
			anc = site.ancestral
			der = site.derived
		else:
			nnoanc += 1
			continue

		titv = ti_or_tv(anc, der)
		if titv == 0:
			nti += 1
		elif titv == 1:
			ntv += 1
		sw = strong_or_weak(anc, der)
		bgc = is_bgc(anc, der)
		if bgc == 1:
			nbgc += 1
		site.INFO["Transversion"] = titv
		site.INFO["StrongWeak"] = sw
		site.INFO["BGC"] = bgc

		site.write_out()

	vcf.add_counter("nnotsnv","sites not SNVs", nnotsnv)
	vcf.add_counter("nnotsnv","sites with >2 alleles", nmulti)
	vcf.add_counter("nbad","sites with bad allele(s)", nbadallele)
	vcf.add_counter("nti","transitions", nti)
	vcf.add_counter("ntv","transversions", ntv)
	vcf.add_counter("nbgc","gBGC candidates", ngbc)
	vcf.show_count_summary()
	vcf.log.info("\t[ TiTv = {:0.3f} ]".format(nti/ntv))

def count_derived(vcf, **kwargs):

	args = argparse.Namespace(**kwargs)
	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	vcf.log.info("Counting derived alleles at level of {}".format(how_count))

	## initialize counters
	what_type = np.int if args.calls else np.float
	counts = np.zeros(len(vcf.samples), dtype = what_type)
	nsites = np.zeros(len(vcf.samples), dtype = np.int)

	## loop on VCF
	vcf.add_counter("nused","sites had usable data")
	vcf.add_counter("nambig","sites had absent ancestral allele")
	vcf.add_counter("nmulti","sites had >2 alleles")
	if args.reads:
		vcf.add_counter("nuncountable","sites had no WSAF annotations")
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		## absent or bogus ancestral allele?
		if not site.has_ancestral:
			vcf.counters["nambig"].increment()
		anc = site.ancestral

		## not biallelic?
		if len(site.ALT) > 1:
			vcf.counters["nmult"].increment()
			continue

		## biallelic, has non-bogus ancestral allele
		if args.reads:
			if not site.has_format("WSAF"):
				vcf.counters["nuncountable"].increment()
				continue
			else:
				gt = site.get_wsaf()
				if not site.REF == anc:
					gt = 1 - gt
				nsites[ ~gt.mask ] += 1
				counts[ ~gt.mask ] += gt[ ~gt.mask ]
				vcf.counters["nused"].increment()
		else:
			gt = site.to_n_der()
			nsites[ ~gt.mask ] += 1
			counts[ ~gt.mask ] += gt[ ~gt.mask ]
			vcf.counters["nused"].increment()

	if args.calls:
		counts = counts/vcf.ploidy
	for jj,iid in enumerate(vcf.samples):
		print(iid, counts[jj], nsites[jj], counts[jj]/nsites[jj], ploidy, sep = "\t")

	vcf.show_count_summary()

def flag_private(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## identify private alleles by hard calls or read counts?
	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	what_count = "READS" if args.reads else "CHROMOSOMES"
	vcf.log.info("Defining private alleles at level of {}".format(how_count))
	vcf.log.info("Cutoff for allele presence: {} supporting {}".format(args.min_count, what_count))

	## decide which samples to use
	keep_samples = vcf.reconcile_samples(args.samples, args.samples_file)
	nsamples = len(keep_samples)

	## get sample positions in input VCF
	focal_idx = [ vcf.samples.index(_) for _ in keep_samples ]
	nonfocal_idx = [ _ for _,iid in enumerate(vcf.samples) if not iid in keep_samples ]

	def is_private(counts):
		if args.reads:
			counts[ counts < 0 ] = 0
		else:
			counts[ counts == 3 ] = 0
		ac_in = np.sum(counts[focal_idx])
		ac_out = np.sum(counts[nonfocal_idx])
		return ac_in >= args.min_count and ac_out < args.min_count

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": args.flag, "Description": "Private allele in specified subset", "Type": "Flag", "Number": "1"})
	vcf.write_header()

	vcf.add_counter("nused","sites had usable data")
	vcf.add_counter("nnodp","sites had no AD annotations")
	vcf.add_counter("nmulti","sites had >2 alleles")
	vcf.add_counter("npriv","private sites")
	vcf.log.info("Processing VCF file ...")
	for site in vcf:

		## not biallelic?
		if len(site.ALT) > 1:
			vcf.counters["nmulti"].increment()
			continue

		if args.reads:
			if not site.has_format("AD"):
				vcf.counters["nnodp"].increment()
				continue
			else:
				vcf.counters["nused"].increment()
			counts = site.format("AD")[:,1]
		else:
			vcf.counters["nused"].increment()
			counts = site.gt_types[:]

		if is_private(counts):
			vcf.counters["npriv"].increment()
			site.INFO[args.flag] = True

		site.write_out()

	vcf.show_count_summary()

def calc_sfs(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## set RNG seed
	if args.seed >= 0:
		vcf.log.info("RNG seed := {}".format(args.seed))
		np.random.seed(args.seed)
	else:
		vcf.log.info("RNG seed := auto [!! NON-REPRODUCIBLE !!]")

	## calculate SFS by hard calls or read counts?
	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	if args.reads:
		if args.use_gp:
			how_count = "HAPLOID GENOTYPE POSTERIORS"
		else:
			how_count = "READ COUNTS"
	else:
		how_count = "GENOTYPE CALLS"

	vcf.log.info("Calculating uSFS from {}".format(how_count))

	pop_order, pops = vcf.reconcile_populations(args.pops)
	ssz = [ len(pops[_]) for _ in pop_order ]
	dims = tuple([ _ + 1 for _ in ssz ])

	vcf.log.info("Populations and their sizes:")
	for ii,pop in enumerate(pop_order):
		vcf.log.info("\t{}  {: >4d}".format(pop, ssz[ii]))

	vcf.log.info("SFS will have these dimensions: {}".format(dims))

	def ivl_to_region(ivl):
		return "{}:{}-{}".format(ivl.chrom, ivl.start+1, ivl.end), ivl.name

	## loop on VCF
	if args.windows is None:
		regions = [ (None,None) ]
		vcf.log.info("Processing VCF file in one slurp ...")
	else:
		regions = map(ivl_to_region, pbt.BedTool(args.windows))
		vcf.log.info("Processing VCF file in chunks defined in <{}> ...".format(args.windows.name))
	all_sfs = OrderedDict()

	vcf.add_counter("nused","had usable data")
	vcf.add_standard_counter("noancestral")
	if args.reads:
		if args.use_gp:
			vcf.add_standard_counter("nowsaf")
		else:
			vcf.add_counter("nogp","did not have genotype posteriors (FORMAT/GP)")
	vcf.add_standard_counter("multiallelic")

	ploidy = None
	nr = 0
	for region,rname in regions:

		nr += 1
		if rname is None or rname == ".":
			rname = "region{:06d}".format(nr)

		## initialize SFS
		sfs = np.zeros(dims, dtype = np.int)

		for site in vcf.fetch_region(region):

			if ploidy is None:
				ploidy = vcf.ploidy
				vcf.log.info("Detected ploidy = {} in this VCF".format(ploidy))

			#if site.is_multiallelic or (not site.has_ancestral and not args.ref_is_ancestral):
			#	continue

			## not biallelic?
			if len(site.ALT) > 1:
				vcf.counters["nmulti"].increment()
				continue

			## no ancestral allele?
			if not site.has_ancestral and not args.ref_is_ancestral:
				vcf.counters["nnoanc"].increment()
				continue

			if args.reads:
				if not args.use_gp:
					if not site.has_format("WSAF"):
						vcf.counters["nowsaf"].increment()
						continue
					else:
						vcf.counters["nused"].increment()
					counts = site.get_wsaf()
					imp_counts, _ = site.get_imputed_wsaf()
					sampled = np.zeros(len(ssz), dtype = np.int)
					for jj,pop in enumerate(pop_order):
						these_counts = counts[ pops[pop] ]
						if these_counts.count == 0:
							## all missing values
							vcf.log.debug("site {}@{}[{}/{}] totally missing".format(site.CHROM, site.POS, site.REF, site.ALT[0]))
							continue
						these_counts = imp_counts[ pops[pop] ]
						if site.alt_is_ancestral:
							these_counts = 1 - these_counts
						if np.min(these_counts < 0) or np.max(these_counts > 1):
							vcf.log.debug("bad imputed WSAFs at {}@{}[{}/{}]: {}".format(site.CHROM, site.POS, site.REF, site.ALT[0], these_counts))
							continue
						sampled[jj] = np.sum(np.random.binomial(1, these_counts))
					sampled = tuple(sampled)
				else:
					if not site.has_format("GP"):
						vcf.counters["nogp"].increment()
						continue
					else:
						vcf.counters["nused"].increment()
					gps = site.format("GP")
					if site.alt_is_ancestral:
						counts_anc = gps[ :,1 ]
						counts_der = gps[ :,0 ]
					else:
						counts_anc = gps[ :,0 ]
						counts_der = gps[ :,1 ]
					sampled = np.zeros(len(ssz), dtype = np.int)
					for jj,pop in enumerate(pop_order):
						sampled[jj] = np.round(np.sum(counts_der[ pops[pop] ])).astype(int)
					sampled = tuple(sampled)
			else:
				vcf.counters["nused"].increment()
				#print(site.REF, site.ALT, site.has_ancestral, site.ancestral, site.derived)
				counts = site.to_n_der()/ploidy
				sampled = np.zeros(len(ssz), dtype = np.int)
				for jj,pop in enumerate(pop_order):
					these_counts = counts[ pops[pop] ]
					maf = np.average(these_counts)
					if np.all(these_counts.mask):
						## all missing values
						continue
					these_counts[ these_counts.mask ] = maf
					sampled[jj] = np.sum(np.random.binomial(1, these_counts))
				sampled = tuple(sampled)

			## finally, update the sfs
			sfs[ sampled ] += 1

		## done with inner loop; record this sfs
		if region is None:
			vcf.log.info(" ... finished region [whole VCF]; cumulative total {} sites [{} used].".format(vcf.ii, vcf.counters["nused"].count))
		else:
			vcf.log.info(" ... finished region {} (group '{}'); cumulative total {} sites [{} used].".format(region, rname, vcf.ii, vcf.counters["nused"].count))

		if rname in all_sfs:
			all_sfs[rname] += sfs
		else:
			all_sfs[rname] = sfs

	vcf.show_count_summary()

	vcf.log.info("Yielding {} spectra from {} regions.".format(len(all_sfs.keys()), nr))
	print("#pops={}".format(",".join(pop_order)))
	print("#dims={}".format(",".join([ str(_) for _ in dims ])))
	print("#nsites={}".format( vcf.counters["nused"].count ))
	for group_name, this_sfs in all_sfs.items():
		print(*this_sfs.flatten(), sep = " ")

## calculate posterior prob of haploid genotype given PLAF and a (default-weak) beta prior
def calc_haploid_gp(acounts, plaf, weight = 1):

	a = acounts[:,0]
	b = acounts[:,1]

	alpha = 1 + weight*plaf
	beta  = 1 + weight*(1-plaf)

	alpha_p = alpha + a
	beta_p  = beta + b

	denom = np.log10(alpha_p + beta_p)
	logpost_a = np.log10(alpha_p) - denom
	logpost_b = np.log10(beta_p) - denom

	return logpost_a, logpost_b

def haplocall(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	vcf.add_format_to_header({ "ID": "GP", \
							"Description": "Haploid genotype posterior given read counts and PLAFs (plus prior with weight={})".format(args.prior_weight), \
							"Number": "G", "Type": "Float" })

	## write outgoing VCF header
	vcf.write_header()
	vcf.add_counter("nkept","sites kept")
	vcf.add_counter("noad","sites without AD annotation")
	vcf.add_standard_counter("multiallelic")
	vcf.log.info("Generating pseudo-haploid calls by taking consensus allele in each sample at each site ...")
	vcf.log.info("Requiring >= {} supporting reads to make a genotype call ...".format(args.min_reads))
	for site in vcf:

		if not "AD" in site.FORMAT or not "DP" in site.FORMAT:
			vcf.counters["noad"].increment()
			continue

		if site.is_multiallelic:
			continue

		vcf.counters["nkept"].increment()

		## get AD values and take argmax
		acounts = site.format("AD")
		consensus = np.argmax(acounts, 1)
		maxreads = np.max(acounts, 1)
		consensus[ maxreads < args.min_reads ] = -1

		## calculate haploid genotype posteriors
		w, plaf = site.get_imputed_wsaf()
		post_ref, post_alt = calc_haploid_gp(acounts, plaf, args.prior_weight)
		#print(post_ref, post_alt)
		post_both = np.column_stack((post_ref, post_alt))
		phred_both = np.round(-10.0*post_both)
		#phred_ordered = np.sort(phred_both, axis = 1).astype(np.int) # NOPE, always same order
		#phred_ordered = phred_both.astype(int) # NOPE, not Phred-scaled
		#phred_ordered = np.column_stack( (phred_ordered[:,0], np.full(phred_ordered.shape[0], 99, dtype = np.int), phred_ordered[:,1]) )
		phred_ordered = 10.0**(-1*phred_both/10.0)
		phred_ordered /= np.sum(phred_ordered, 1, keepdims = True)
		#phred_ordered = np.column_stack( (phred_ordered[:,0], np.zeros(phred_ordered.shape[0]), phred_ordered[:,1]) )

		## convert missing to "."
		final_call = [ "." if _ == -1 else "{0}".format(_) for _ in consensus ]
		#final_call = [ "./." if _ == -1 else "{0}/{0}".format(_) for _ in consensus ]
		final_ad = [ ",".join( str(_) for _ in x ) for x in acounts ]
		final_dp = [ str(_[0]) for _ in site.format("DP") ]
		#final_gp = [ ",".join( "{:0d}".format(_) for _ in x ) for x in phred_ordered ]
		final_gp = [ ",".join( "{:0.3f}".format(_) for _ in x ) for x in phred_ordered ]

		fields = str(site).strip().split("\t")[:8]
		fields += [ "GT:AD:DP:GP" ]
		fields += [ ":".join(_) for _ in zip(final_call, final_ad, final_dp, final_gp) ]
		print(*fields, sep = "\t")

	vcf.show_count_summary()

def thin_sites(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## write outgoing VCF header
	vcf.write_header()

	vcf.add_counter("nkept","sites kept")
	vcf.add_counter("ndropped","sites discarded")
	vcf.log.info("Processing VCF file ...")
	for site in vcf:
		if not vcf.ii % args.every:
			vcf.counters["nkept"].increment()
			site.write_out()
		else:
			vcf.counters["ndropped"].increment()

	## finish up
	vcf.show_count_summary()

def prune_sites_by_ld(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## write outgoing VCF header
	vcf.write_header()

	vcf.log.info("Performing LD pruning to r^2 <= {:0.3f} in windows of {} sites (stepping by {})".format(args.r_squared, args.window, args.step))
	## straight from <http://alimanfoo.github.io/2015/09/28/fast-pca.html>
	def ld_prune(gn, size, step, threshold = 0.1, n_iter = 1):
		idx = np.arange(gn.shape[0])
		for i in range(n_iter):
			loc_unlinked = locate_unlinked(gn, size=size, step=step, threshold=threshold)
			n = np.count_nonzero(loc_unlinked)
			n_remove = gn.shape[0] - n
			vcf.log.info("\t ... iteration {}: retaining {} of {} sites".format(i, n, gn.shape[0]))
			gn = gn[ loc_unlinked,... ]
			idx = idx[ loc_unlinked ]
		return idx

	## read genotypes to matrix, one chromosome at a time
	nkept = 0
	vcf.add_counter("ntot","sites retained")
	vcf.add_counter("ndropped","sites pruned for LD")
	vcf.log.info("Processing VCF file ...")
	last_chrom = None
	pos = []
	geno = vcf.add_chunk(None, args.chunk_size, np.float)

	## while instead of for-loop to avoid gymnasitcs at ends of chrosomes
	# while True:
	#
	# 	# consume next site
	# 	try:
	# 		site = next(vcf)
	# 	except StopIteration as e:
	# 		break

	for site in vcf:

		# increment counters
		nkept += 1

		# increase array allocation, if needed
		if nkept >= geno.shape[0]:
			vcf.log.debug("Increasing array allocation {} --> {} ... ".format(geno.shape[0], geno.shape[0]+args.chunk_size))
			geno = vcf.add_chunk(geno, args.chunk_size)

		if last_chrom and last_chrom != site.CHROM:
			# started new chromosome; calculate LD, reset containers and keep going
			vcf.log.info("Pruning {} sites on chromosome '{}'".format(nkept, last_chrom))
			keep = ld_prune(geno[:(nkept-1),:], args.window, args.step, args.r_squared, args.niter)
			vcf.log.debug("{} {} {}".format(nkept-1, np.max(keep), len(pos)))
			#vcf.counters["ndropped"].increment( (nkept-1)-len(keep) )
			for jj in keep:
				vcf.counters["ntot"].increment()
				pos[jj].write_out()
			geno = vcf.add_chunk(None, args.chunk_size)
			pos = []
			nkept = 0
			last_chrom = str(site.CHROM)
			continue

		last_chrom = str(site.CHROM)
		geno[nkept,:] = site.gt_types/2 * site.ploidy
		geno[nkept, site.gt_types == 3 ] = np.nan
		pos.append(site)

	## done with file; flush buffers
	vcf.log.info("Pruning {} sites on chromosome '{}'".format(nkept, last_chrom))
	keep = ld_prune(geno[:nkept,:], args.window, args.step, args.r_squared, args.niter)
	vcf.log.debug("{} {} {}".format(nkept, np.max(keep), len(pos)))
	#vcf.counters["ndropped"].increment( nkept-len(keep) )
	for jj in keep:
		vcf.counters["ntot"].increment()
		pos[jj].write_out()

	## finish up
	vcf.counters["ndropped"].increment( vcf.ii - vcf.counters["ntot"].count )
	vcf.show_count_summary()


def get_threepop_combos(x, outgroup = None):

	if outgroup is None:
		seen = []
		for a,b,c in permutations(x, 3):
			if (a,c,b) in seen:
				continue
			else:
				seen.append( (a,b,c) )
		return seen
	else:
		ingroups = list(set(x)-set([outgroup]))
		combos = []
		for a,b in combinations(ingroups, 2):
			combos.append( (outgroup, a, b) )
		return combos

def get_fourpop_combos(x):

	seen = []
	for a,b,c,d in permutations(x, 4):
		if (b,a,c,d) in seen:
			# permute first pair
			continue
		elif (a,b,d,c) in seen:
			# permute second pair
			continue
		elif (b,a,d,c) in seen:
			# permute both pairs
			continue
		else:
			seen.append( (a,b,c,d) )
	return seen

def calc_f3(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	vcf.log.info("Counting derived alleles at level of {}".format(how_count))

	## read in populations
	pop_order, pops = vcf.reconcile_populations(args.pops)
	npops = len(pop_order)
	if len(pop_order) < 3:
		vcf.log.error("Need >= 3 populations to calculate f_3 statistics; only {} provided.".format(len(pop_order)))
		sys.exit(1)

	## get tuples representing 3-population combinations where order of last 2 doesn't matter
	##    since f(C;A,B) == f(C;B,A)
	if args.outgroup and args.outgroup in pop_order:
		og = args.outgroup
		vcf.log.info("Holding population '{}' as outgroup.".format(og))
	else:
		og = None
	combos = get_threepop_combos(pop_order, og)
	ncombos = len(combos)
	vcf.log.info("There will be this many f_3 statistics to compute: {}.".format(ncombos))

	freqs = vcf.add_chunk(None, args.chunk_size, nsamples = npops)

	## loop on sites
	nkept = 0
	vcf.add_counter("nused","sites had usable data")
	vcf.add_standard_counter("multiallelic")
	vcf.add_counter("nnodp","sites had no WSAF annotations")
	for site in vcf:

		# increase array allocation, if needed
		if vcf.ii >= freqs.shape[0]:
			vcf.log.debug("Increasing array allocation {} --> {} ... ".format(freqs.shape[0], freqs.shape[0]+args.chunk_size))
			freqs = vcf.add_chunk(freqs, args.chunk_size, nsamples = npops)

		if site.is_multiallelic:
			continue

		if args.reads:
			wsaf = site.get_wsaf()
		elif args.calls:
			wsaf = site.to_n_alt()/vcf.ploidy

		if np.isnan(np.sum(wsaf)):
			vcf.counters["nnodp"].increment()
			continue

		nkept += 1
		## calculate allele freqs for each population; will use them repeatedly
		for jj,pop in enumerate(pop_order):
			freqs[ nkept, jj ] = np.average(wsaf[ pops[pop] ])

		#vcf.log.debug(freqs)

	vcf.counters["nused"].increment( nkept )
	vcf.show_count_summary()

	vcf.log.info("Estimating f_3 and its standard errors by block jackknife in blocks of size ~= {}".format(args.jackknife))

	def estimate_f3(data):

		fstats = np.tile(np.nan, len(combos))
		## loop on population tuples and calculate numerator and denominator of f-statistic
		for jj,(c,a,b) in enumerate(combos):
			cprime = data[ :,pop_order.index(c) ]
			aprime = data[ :,pop_order.index(a) ]
			bprime = data[ :,pop_order.index(b) ]
			Ti = (cprime - aprime)*(cprime - bprime)
			Bi = 2*cprime*(1 - cprime)
			num = np.sum(np.ma.masked_invalid(Ti), 0)
			den = np.sum(np.ma.masked_invalid(Bi), 0)
			fstats[jj] = num/den

		return fstats

	se, _ = block_jackknife(freqs, estimate_f3, args.jackknife)
	f3s = estimate_f3(freqs)
	zscores = f3s/se
	formatter = "{:0.5f}"
	for jj,(C,A,B) in enumerate(combos):
		print(C, A, B, formatter.format(f3s[jj]), formatter.format(se[jj]), formatter.format(zscores[jj]), sep = "\t")

	vcf.log.info("Done.")

def calc_f4(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	vcf.log.info("Counting derived alleles at level of {}".format(how_count))

	## read in populations
	pop_order, pops = vcf.reconcile_populations(args.pops)
	npops = len(pop_order)
	if len(pop_order) < 4:
		vcf.log.error("Need >= 4 populations to calculate f_4 statistics; only {} provided.".format(len(pop_order)))
		sys.exit(1)

	## get tuples representing 4-population combinations where order of last 2 doesn't matter
	##    since f(W,X;A,B) == f(X,W;B,A) etc
	combos = get_fourpop_combos(pop_order)
	ncombos = len(combos)
	vcf.log.info("There will be this many f_4 statistics to compute: {}.".format(ncombos))

	freqs = vcf.add_chunk(None, args.chunk_size, nsamples = npops)

	## loop on sites
	nkept = 0
	vcf.add_counter("nused","sites had usable data")
	vcf.add_standard_counter("multiallelic")
	vcf.add_counter("nnodp","sites had no WSAF annotations")
	for site in vcf:

		# increase array allocation, if needed
		if vcf.ii >= freqs.shape[0]:
			vcf.log.debug("Increasing array allocation {} --> {} ... ".format(freqs.shape[0], freqs.shape[0]+args.chunk_size))
			freqs = vcf.add_chunk(freqs, args.chunk_size, nsamples = npops)

		if site.is_multiallelic:
			continue

		if args.reads:
			wsaf = site.get_wsaf()
		elif args.calls:
			wsaf = site.to_n_alt()/vcf.ploidy

		if np.isnan(np.sum(wsaf)):
			vcf.counters["nnodp"].increment()
			continue

		nkept += 1
		## calculate allele freqs for each population; will use them repeatedly
		for jj,pop in enumerate(pop_order):
			freqs[ nkept, jj ] = np.average(wsaf[ pops[pop] ])

		#vcf.log.debug(freqs)

	vcf.counters["nused"].increment( nkept )
	vcf.show_count_summary()

	vcf.log.info("Estimating f_4 and its standard errors by block jackknife in blocks of size ~= {}".format(args.jackknife))

	def estimate_f4(data):

		fstats = np.tile(np.nan, len(combos))
		## loop on population tuples and calculate numerator and denominator of f-statistic
		for jj,(w,x,y,z) in enumerate(combos):
			wprime = data[ :,pop_order.index(w) ]
			xprime = data[ :,pop_order.index(x) ]
			yprime = data[ :,pop_order.index(y) ]
			zprime = data[ :,pop_order.index(z) ]
			Ti = (wprime - xprime)*(yprime - zprime)
			Bi = (wprime + xprime - 2*wprime*xprime)*(yprime + zprime - 2*yprime*zprime)
			num = np.sum(np.ma.masked_invalid(Ti), 0)
			den = np.sum(np.ma.masked_invalid(Bi), 0)
			fstats[jj] = num/den

		return fstats

	se, _ = block_jackknife(freqs, estimate_f4, args.jackknife)
	f4s = estimate_f4(freqs)
	zscores = f4s/se
	formatter = "{:0.5f}"
	for jj,(W,X,Y,Z) in enumerate(combos):
		print(W,X,Y,Z, formatter.format(f4s[jj]), formatter.format(se[jj]), formatter.format(zscores[jj]), sep = "\t")

	vcf.log.info("Done.")

def make_treemix(vcf, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	if not (args.reads or args.calls) or (args.reads and args.calls):
		vcf.log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	vcf.log.info("Counting alleles at level of {}".format(how_count))

	## read in populations
	pop_order, pops = vcf.reconcile_populations(args.pops)
	if len(pop_order) < 3:
		vcf.log.error("Need >= 3 populations for TreeMix; only {} provided.".format(len(pop_order)))
		sys.exit(1)

	## loop on sites
	nkept = 0
	vcf.add_counter("nused","sites had usable data")
	vcf.add_standard_counter("multiallelic")
	vcf.add_standard_counter("nowsaf")

	print(*pop_order, sep = "\t")
	for site in vcf:

		if site.is_multiallelic or (args.reads and not site.has_wsaf):
			continue

		vcf.counters["nused"].increment()

		## calculate allele freqs for each population; will use them repeatedly
		freqs = []
		if args.reads:
			counts = site.get_wsaf()
		else:
			counts = site.to_n_alt()

		for pop in pop_order:
			if args.reads:
				aaf = np.average(counts[ pops[pop] ])
				if aaf is np.ma.masked:
					aaf = 0
					raf = 0
				else:
					raf = 1.0 - aaf
				freqs.append( (aaf,raf) )
			else:
				aaf = np.sum(counts[ pops[pop] ])
				if aaf is np.ma.masked:
					aaf = 0
					raf = 0
				else:
					raf = site.ploidy*np.sum(~counts[ pops[pop] ].mask) - aaf
				#vcf.log.debug(aaf)
				freqs.append( (int(aaf), int(raf)) )

		print(*[ "{},{}".format(a,b) for a,b in freqs ], sep = "\t")

	vcf.show_count_summary()

## root-level argument parser
def root_help_message(name = None):
	return r"""
	vcfdo <command> [options]

	Commands:

	  --- Basic statistics
	      missing           rates of missing calls by site or by sample
	      depthsummary      quantiles of read depth (at called sites) per sample
	      filtersummary     tallies of filter status by site or by sample

	  --- Downsampling
	      thin              just emit every nth site
	      prune             greedy LD-pruning in sliding windows

	  --- Ancestral alleles
	      polarize          define ancestral alleles based on outgroup sequence
	      titv              annotate sites as transitions or transversions; flag gBGC candidates
	      derived           count derived alleles per sample, using either read counts or hard calls

	  --- Allele frequencies
	      wsaf              calculate within-sample allele frequency (WSAF) and related quantities from read counts
	      fws               estimate pseudo-inbreeding coefficient F_ws ("within-sample relatedness")
	      wsafhist          calculate histogram of WSAFs within samples for QC
	      private           annotate sites where non-ref allele is only found in certain subset of samples
	      sfs               approximate the n-dimensional unfolded SFS by sampling, possibly in windows

	  --- Genotyping
	      haplocall         convert diploid to pseudo-haploid call by taking consensus allele at each site

	  --- Relatedness/ordination
	      dist              calculate LD-weighted pairwise distances from WSAFs
	      ibs               calculate simple identity-by-state (IBS) matrix from hard calls at polymorphic sites only
	      ibd               calculate identity-by-descent (IBD) matrix from WSAFs and Malecot's (1970) definition of IBD
	      pca               perform PCA using within-sample allele frequencies instead of hard calls

	  --- Admixture
	      f3stat            calculate Patterson's normalized f_3 statistic
	      f4stat            calculate Patterson's normalized f_4 statistic, also known as the D-statistic
	      treemix           make input file for `TreeMix` software

	Notes:
	* All commands that take VCF as input or output work on pipes. For basic filtering operations,
	it is recommended to use `bcftools` and pipe to this tool.
	* Input can be in any format that `htslib` can handle -- VCF, bgzipped VCF, BCF, etc.
	* Commands which modify the VCF will add an entry to the header recording the command line used,
	the current working directory and a timestamp.
	"""

parser = argparse.ArgumentParser(
	description = "Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.",
	usage = root_help_message(),
	add_help = False)
subparsers = parser.add_subparsers()
parser.add_argument("-i","--infile",
					default = "/dev/stdin",
					help = "VCF file, possibly gzipped [default: stdin]")
parser.add_argument("-n","--nsites", type = int,
					help = "stop after processing this many sites [default: no limit]")
parser.add_argument("-t","--threads", type = int,
					default = None,
					help = "number of threads to use for VCF parsing [default: no threading]")
parser.add_argument("--ref-is-ancestral", action = "store_true",
					default = False,
					help = "assume reference allele is the ancestral one, ignoring INFO/AA (USE WITH CAUTION)")
parser.add_argument("--chunk-size", type = int,
					default = 2000,
					help = "when loading genotypes into memory, process this many sites per chunk [default: %(default)d]")
parser.add_argument("--seed", type = int,
					default = 1,
					help = "random number seed for reproducibility, values < 0 use auto seed; only affects stuff involving sampling [default: %(default)d]")
parser.add_argument("-q","--quiet", action = "store_true",
					help = "see fewer logging messages")
parser.add_argument("-v","--verbose", action = "store_true",
					help = "see more logging messages")

## calculate WSAF, PLAF, PLMAF
wsaf_parser = subparsers.add_parser("wsaf",
	description = "Calculate within-sample allele frequencies (WSAF) from read counts, and their population-level equivalents.",
	parents = [parser])
wsaf_parser.add_argument("-d","--min-depth","--min-DP", type = int,
						dest = "min_depth",
						default = 0,
						help = "minimum read depth for calculating WSAF [default: %(default)d]")
wsaf_parser.add_argument("--min-GQ", type = int,
						default = 0,
						help = "minimum GQ for calculating WSAF [default: %(default)d]")
wsaf_parser.add_argument("--ignore-missing", action = "store_true",
						default = False,
						help = "don't calculate WSAF when no hard genotype call was made [default: ignore hard calls]")
wsaf_parser.add_argument("--set-filter", type = float,
						default = None,
						help = "apply genotype-level filter for samples with WSAF outside specified bounds [default: don't do this]")
wsaf_parser.set_defaults(func = calc_wsaf, command = "wsaf")

## calculate F_ws from WSAF
fws_parser = subparsers.add_parser("fws",
	description = "Calculate within-sample allele 'inbreeding'-like coefficient F_ws, using precomputed WSAFs. Optionally get standard errors by block jackknife.",
	parents = [parser])
fws_parser.add_argument("-j","--jackknife", type = int,
					default = None,
					help = "estimate standard errors by block jackknife with this block size (number of sites) [default: use asymptotics]")
fws_parser.set_defaults(func = calc_fws, command = "fws")

## calculate WSAF, PLAF, PLMAF
wsaf_parser = subparsers.add_parser("wsafhist",
	description = "Calculate histogram of within-sample allele frequencies (WSAF) in each sample.",
	parents = [parser])
wsaf_parser.add_argument("-b","--binwidth", type = float,
						default = 0.05,
						help = "width of histogram bins [default: %(default)f]")
wsaf_parser.set_defaults(func = calc_wsaf_hist, command = "wsafhist")

## summarise depth per sample per called site
dp_parser = subparsers.add_parser("depthsummary",
	description = "Summarise depth (FORMAT:DP) per site per sample, splitting by filter status of sites.",
	parents = [parser])
dp_parser.set_defaults(func = summarise_dp, command = "depthsummary")

## summarise filters at site or individual level
flt_parser = subparsers.add_parser("filtersummary",
	description = "Tally filter status of sites or samples.",
	parents = [parser])
flt_parser.add_argument("--sites", action = "store_true",
						help = "tally filter status of sites")
flt_parser.add_argument("--samples", action = "store_true",
						help = "tally filter status of per-sample genotypes")
flt_parser.set_defaults(func = summarise_flt, command = "filtersummary")

## summarise filters at site or individual level
miss_parser = subparsers.add_parser("missing",
	description = "Tally missing calls per site or per sample.",
	parents = [parser])
miss_parser.add_argument("--sites", action = "store_true",
						help = "tally missingness per site")
miss_parser.add_argument("--samples", action = "store_true",
						help = "tally missingness per sample, across all sites")
miss_parser.set_defaults(func = calc_missingness, command = "missing")

## do PCA on WSAFs
pca_parser = subparsers.add_parser("pca",
	description = "Perform PCA on with-sample allele frequencies instead of genotype calls.",
	parents = [parser])
pca_parser.add_argument("-o","--out",
					default = "wsaf_pca",
					help = "prefix for a pair of output files, *.pca and *.pca.ev [default: %(default)s]")
pca_parser.add_argument("-s","--samples", nargs = "+",
					help = "restrict analysis to these samples, whitespace-separated [default: use all samples]")
pca_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "restrict analysis to samples in this file, one per line [default: use all samples]")
pca_parser.add_argument("--project", type = argparse.FileType("rU"),
					help = "calculate PCs on samples in this file, then project remaining samples onto that space [default: one-pass PCA on all samples]")
pca_parser.add_argument("-f","--min-plmaf", type = float,
					default = 0.0,
					help = "limit to sites with PMLAF > {min_plmaf} [default: %(default)f]")
pca_parser.add_argument("-m","--max-missing", type = float,
					default = 1.0,
					help = "limit to sites with less than this proportion missing WSAF [default: %(default)f]")
pca_parser.add_argument("-K","--ncomponents", type = int,
					default = 10,
					help = "calculate the first K PCs if using randomized SVD algorithm [default: %(default)d]")
pca_parser.add_argument("--randomized", action = "store_true",
					help = "use fast randomized SVD algorithm")
pca_parser.set_defaults(func = pca_wsaf, command = "pca")

## genetic distance from WSAFs, weighted for LD
dist_parser = subparsers.add_parser("dist",
	description = "Calculate LD-weighted genetic distances from within-sample allele frequencies according to method of Amato et al. (PMC4786412).",
	parents = [parser])
dist_parser.add_argument("-d","--digits", type = int,
					default = 5,
					help = "report distance to this many decimal places [default: %(default)d]")
dist_parser.add_argument("-r","--rsq-cutoff", type = float,
					default = 0.10,
					help = "ignore site pairs with r^2 < cutoff when assigning weights [default: %(default)f]")
dist_parser.add_argument("-u","--unweighted", action = "store_true",
					help = "calculated distance from WSAFs but ignore LD")
dist_parser.set_defaults(func = calc_weighted_dist, command = "dist")

## quick identity-by-state on hard genotypes
ibs_parser = subparsers.add_parser("ibs",
	description = "Calculate identity-by-state (IBS) matrix from hard genotype calls, without weights (but only polymorphic sites).",
	parents = [parser])
ibs_parser.add_argument("-d","--digits", type = int,
					default = 3,
					help = "report IBS to this many decimal places [default: %(default)d]")
ibs_parser.add_argument("--flatten", action = "store_true",
					help = "report one pair of individuals per line, rather than square matrix")
ibs_parser.set_defaults(func = calc_ibs_dist, command = "ibs")

## quick identity-by-state on hard genotypes
ibd_parser = subparsers.add_parser("ibd",
	description = "Calculate identity-by-descent (IBD) matrix from WSAFs. Assumes Malecot (1970) definition of IBD and ideas from Bob Verity.",
	parents = [parser])
ibd_parser.add_argument("--self-only", action = "store_true",
					help = "estimate within-sample IBD (ie. inbreeding coefficients) instead of pairwise")
ibd_parser.add_argument("-w","--wsaf-threshold", type = float,
					default = 0.01,
					help = "minimum WSAF to consider evidence for >1 allele when `--self-only` [default: %(default)f]")
ibd_parser.add_argument("--tol", type = float,
					default = 1e-4,
					help = "error tolerance for estimation of IBD coefficients [default: %(default)f]")
ibd_parser.add_argument("--max-iter", type = int,
					default = 250,
					help = "maximum number of iterations for numerical optimization [default: %(default)d]")
ibd_parser.add_argument("-d","--digits", type = int,
					default = 3,
					help = "report IBS to this many decimal places [default: %(default)d]")
ibd_parser.set_defaults(func = calc_malecot_ibd, command = "ibd")

## polarize alleles using ancestral/outgroup genome in fasta format
anc_parser = subparsers.add_parser("polarize",
	description = "Polarize alleles as ancestral vs derived, based on outgroup sequence",
	parents = [parser])
anc_parser.add_argument("-f","--fasta",
					required = True,
					help = "ancestral genome fasta file, **in same coordinates as reference**")
anc_parser.set_defaults(func = polarize_anc_vs_der, command = "polarize")

## annotate sites as transition or transversion, gBGC or not, and biochemical "strength"
anc_parser = subparsers.add_parser("titv",
	description = "Annotate variants as transition or transversion; flag gBGC candidates",
	parents = [parser])
anc_parser.set_defaults(func = transition_or_transversion, command = "titv")

## count number of derived calls per sample
anc_parser = subparsers.add_parser("derived",
	description = "Count proportion of calls for derived vs ancestral allele, using WSAF (default) or hard calls",
	parents = [parser])
anc_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
anc_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
anc_parser.set_defaults(func = count_derived, command = "derived")

## identify variants where non-ref allele is present in only specified subset of samples
priv_parser = subparsers.add_parser("private",
	description = "Annotate sites as 'private' in subset of samples.",
	parents = [parser])
priv_parser.add_argument("-f","--flag",
					default = "PRIV",
					help = "flag to add to INFO field for private sites [default: '%(default)s']")
priv_parser.add_argument("-s","--samples", nargs = "+",
					default = [],
					help = "one or more focal samples")
priv_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "focal samples are listed in this file [overrides `--samples`]")
priv_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
priv_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
priv_parser.add_argument("-c","--min-count", type = int,
					default = 1,
					help = "minimum supporting observations (reads or chromosomes) in focal sample(s) for allele to be called 'present' [default: %(default)d]")
priv_parser.set_defaults(func = flag_private, command = "private")

## calculate n-dimensional SFS
sfs_parser = subparsers.add_parser("sfs",
	description = "Estimate unfolded SFS by sampling from either read counts or hard calls.",
	parents = [parser])
sfs_parser.add_argument("-p","--pops", type = argparse.FileType("rU"),
					help = "file of (sample ID, population) tuples [default: all samples as one population]")
sfs_parser.add_argument("-w","--windows", type = argparse.FileType("rU"),
					help = "estimate SFS in windows from this *.bed file [default: use whole file]")
sfs_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
sfs_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
sfs_parser.add_argument("--use-gp", action = "store_true",
					default = False,
					help = "get allele counts from haploid genotype posteriots (FORMAT/GP), with `--reads` [default: %(default)s]")
sfs_parser.set_defaults(func = calc_sfs, command = "sfs")

haplo_parser = subparsers.add_parser("haplocall",
	description = "Make pseudo-haploid genotype calls based on allele read depths.",
	parents = [parser])
haplo_parser.add_argument("--min-reads", type = int,
					default = 1,
					help = "require at least this many supporting reads to call a genotype [default: %(default)d]")
haplo_parser.add_argument("-w","--prior-weight", type = float,
					default = 1.0,
					help = "weight of beta prior on allele frequencies [default: %(default)d]")
haplo_parser.set_defaults(func = haplocall, command = "haplocall")

## calculate f3 statistic
f3_parser = subparsers.add_parser("f3stat",
	description = "Estimate Patterson's f_3 statistic for gene flow f(C;A,B) (from PMC3522152)",
	parents = [parser])
f3_parser.add_argument("-p","--pops", type = argparse.FileType("rU"),
					required = True,
					help = "file of (sample ID, population) tuples")
f3_parser.add_argument("-o","--outgroup",
					help = "compute so-called 'outroup f_3' holding this population as outgroup [default: all possible f_3s]")
f3_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
f3_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
f3_parser.add_argument("-j","--jackknife", type = int,
					default = 1000,
					help = "estimate standard errors by block jackknife with this block size (number of sites) [default: %(default)d]")
f3_parser.set_defaults(func = calc_f3, command = "f3stat")

## calculate f4 statistic
f4_parser = subparsers.add_parser("f4stat",
	description = "Estimate Patterson's f_4 statistic for gene flow f(W,X;Y,Z) (from PMC3522152)",
	parents = [parser])
f4_parser.add_argument("-p","--pops", type = argparse.FileType("rU"),
					help = "file of (sample ID, population) tuples [default: all samples as one population]")
f4_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
f4_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
f4_parser.add_argument("-j","--jackknife", type = int,
					default = 1000,
					help = "estimate standard errors by block jackknife with this block size (number of sites) [default: %(default)d]")
f4_parser.set_defaults(func = calc_f4, command = "f4stat")

## make TreeMix input file
tmx_parser = subparsers.add_parser("treemix",
	description = "Create input file for `TreeMix` software (cf PMC3499260)",
	parents = [parser])
tmx_parser.add_argument("-p","--pops", type = argparse.FileType("rU"),
					help = "file of (sample ID, population) tuples [default: all samples as one population]")
tmx_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
tmx_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
tmx_parser.set_defaults(func = make_treemix, command = "treemix")

## keep only every nth site
thin_parser = subparsers.add_parser("thin",
	description = "Reduce number of sites in VCF by emitting every nth site",
	parents = [parser])
thin_parser.add_argument("-Q", "--every", type = int,
						default = 10,
						help = "emit one site per this many visited [default: %(default)d]")
thin_parser.set_defaults(func = thin_sites, command = "thin")

## prune for LD
prune_parser = subparsers.add_parser("prune",
	description = "Use greedy window-based method to prune sites in LD",
	parents = [parser])
prune_parser.add_argument("-w", "--window", type = int,
						default = 100,
						help = "window size in number of sites [default: %(default)d]")
prune_parser.add_argument("-s", "--step", type = int,
						default = 50,
						help = "offset between adjacent windows [default: %(default)d]")
prune_parser.add_argument("-r", "--r-squared", type = float,
						default = 0.1,
						help = "retain sites with r^2 below this threshhold [default: %(default)f]")
prune_parser.add_argument("-m", "--niter", type = int,
						default = 5,
						help = "perform this many rounds of pruning in each window before giving up [default: %(default)d]")
prune_parser.set_defaults(func = prune_sites_by_ld, command = "prune")

## parse command line
args = parser.parse_args()

## set up log trace
if args.quiet:
	logging.basicConfig(level = logging.CRITICAL)
elif args.verbose:
	logging.basicConfig(level = logging.DEBUG)
else:
	logging.basicConfig(level = logging.INFO)
logging.StreamHandler(stream = sys.stderr)
logger = logging.getLogger("vcfutils")
logger.debug("Command-line args, as parsed: {}".format(args))
logger.info("Task: {}".format(args.command))

## connect to VCF file
cmd_line = " ".join(sys.argv[1:])
pwd = os.path.expandvars(os.getcwd())
breadcrumb = "vcfdo_command={}; pwd={}".format(cmd_line, pwd)
vcf = core.VcfIterator(args.infile, nsites = args.nsites, log_name = args.command, breadcrumb = breadcrumb, threads = args.threads)
vcf.assume_ref_ancestral = args.ref_is_ancestral

## dispatch
args.func(vcf = vcf, **vars(args))
