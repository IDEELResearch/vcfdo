#! /usr/bin/env python3
"""
vcfdo
Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.
"""

import os
import sys
import numpy as np
import itertools as it
import argparse
import logging

np.warnings.filterwarnings("ignore")

from re import split as resplit
from datetime import datetime as dt
from collections import defaultdict, OrderedDict
from cyvcf2 import VCF
from allel import pca, randomized_pca, pairwise_distance, locate_unlinked
#from allel.stats import rogers_huff_r
from scipy.spatial.distance import squareform
from pyfaidx import Fasta

def block_jackknife(values, statistic, block_size = 1):

	## how long is input? allow to be multidimensional, assume replicates are in rows
	n = values.shape[0]
	#print("values have shape {}".format(values.shape))

	## how many blocks, respecting (approx) the block size?
	m = block_size
	M = int(n/m)+1

	## split into a list of arrays, each item a block
	blocks = np.array_split(np.indices((n,))[0], M)
	blocks = [ _ for _ in blocks if _.shape[0] > 0 ]
	M = len(blocks)

	## not enough blocks to get standard errors? too bad
	if M < 3:
		return np.nan, np.tile(np.nan, M)

	## loop on blocks
	theta = np.zeros(M)
	for jj, delete_j in enumerate(blocks):
		## use mask out this block
		idx = np.tile(True, n)
		#print("To delete = {}".format(delete_j))
		idx[delete_j] = False
		keep = values[ idx,... ]
		#print("Keepers have shape {}".format(keep.shape))
		## calculate statistic on remaining values
		theta[jj] = statistic(keep)
		#print("theta_j = {}".format(theta[jj]))

	## calculate standard error
	theta_bar = np.mean(theta)
	#print("theta_bar = {}".format(theta_bar))
	se = np.sqrt( ((M-1)/M) * np.sum(np.power(theta - theta_bar, 2)) )
	#print("se = {}".format(se))

	## return se and blockwise stats
	return se, theta

def calc_wsaf(vcf, log, ns, **kwargs):

	vcf.add_info_to_header({ "ID": "PLAF", "Description": "Population-level ALT allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "PLMAF", "Description": "Population-level minor allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "UNW", "Description": "Population-level evidence for minor allele, as if all reads were pooled", "Number": 1, "Type": "Integer" })
	vcf.add_format_to_header({ "ID": "WSAF", "Description": "Within-sample allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_format_to_header({ "ID": "WSMAF", "Description": "Within-sample MAJOR allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })

	do_filter = kwargs["set_filter"] is not None
	if do_filter:
		filter_at = min(1 - kwargs["set_filter"], kwargs["set_filter"])
		vcf.add_filter_to_header({ "ID": "PseudoHet", "Description": "WSAF out-of-bounds for a pure isolate" })
		vcf.add_format_to_header({ "ID": "FT", "Description": "Genotype-level filter", "Number": ".", "Type": "String" })

	log.info("Annotating VCF file: <{}>".format(kwargs["infile"]))
	sys.stdout.write(vcf.raw_header)

	## loop on VCF and add 'WSAF','PLAF','PLMAF' tags
	ii = 0
	nmulti = 0
	nnonpoly = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))
		if ii > ns and ns > -1:
			ii -= 1
			break

		## don't bother with multiallelic sites
		if len(site.ALT) > 1:
			nmulti += 1

		## don't bother with nonpolymorphic sites
		if len(site.ALT) < 1:
			nnonpoly += 1

		dp = site.format("AD")
		dp[ dp < 0 ] = 0
		## is depth recorded for >1 allele?
		if dp.shape[1] >= 2:
			# > 1 allele counted
			dp_ref = dp[:,0]
			dp_alt = np.sum(dp[:,1:], 1)
			dp_maj = np.amax(dp, 1)
			dp_tot = np.sum(dp, 1)

			## are there actually any reads for the non-ref alleles?
			dp_tot_all = np.sum(dp)
			dp_alt_all = np.sum(dp[:,1:])
			dp_ref_all = np.sum(dp[:,0])

			wsmaf = dp_maj / dp_tot
			wsaf = np.ma.masked_invalid(dp_alt / (dp_ref + dp_alt), )

			## figure out which allele is minor in population
			## NB: for UNW, record number of reads supporting minor allele -- which may be MORE than major allele,
			##  depending on distribution per-sample coverage. But we want evidence of favor of rarest allele.
			plaf = np.average(wsaf)
			plmaf = -1
			if plaf < 0.5:
				plmaf = plaf
				unw = dp_alt_all
			else:
				plmaf = 1 - plaf
				unw = dp_ref_all
			site.INFO["PLAF"] = float(plaf)
			site.INFO["PLMAF"] = float(plmaf)
			site.INFO["UNW"] = int(unw)

			# now undo masking of NaNs
			wsaf[ np.isnan(wsaf) ] = -1
			wsmaf[ np.isnan(wsmaf) ] = -1
			site.set_format("WSAF", wsaf)
			site.set_format("WSMAF", wsmaf)

		else:
			# only 1 allele counted
			site.INFO["PLAF"] = 0.0
			site.INFO["PLMAF"] = 0.0
			site.set_format("WSAF", np.tile(0.0, len(vcf.samples)))

		if do_filter:
			gt_filt = np.tile("PASS", len(vcf.samples))
			gt_filt[ np.logical_or(wsaf > args.set_filter, wsaf < (1-args.set_filter)) ] = "PseudoHet"
			if "GT" in site.FORMAT:
				old_filt = site.format("FT")
				for ii, flt in enumerate(old_filt):
					if flt != "PASS":
						gt_filt[ii] = flt + ";" + gt_filt[ii]
			site.set_format("FT", gt_filt)

		try:
			sys.stdout.write(str(site))
			#sys.stdout.flush()
		except BrokenPipeError as e:
			log.warning("Stopped because the pipe reading from this program has closed.".format(ii))
			break

	log.info("Done.".format(ii))
	log.info("\t{: >12d} biallelic in called genotypes".format(ii-nmulti))
	log.info("\t{: >12d} non-polymorphic in called genotypes".format(nnonpoly))
	log.info("\t{: >12d} multiallelic".format(nmulti))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))


def calc_fws(vcf, log, ns, **kwargs):

	log.info("Processing VCF file: <{}>".format(kwargs["infile"]))
	args = argparse.Namespace(**kwargs)

	## loop on VCF and add 'WSAF','PLAF','PLMAF' tags
	ii = 0
	nmulti = 0
	nnonpoly = 0
	nnowsaf = 0
	log.info("Processing VCF file ...")
	H_s = []
	H_p = []

	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		## don't bother with multiallelic sites
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		## don't bother with nonpolymorphic sites
		if len(site.ALT) < 1:
			nnonpoly += 1
			continue

		#if (not "WSAF" in site.FORMAT) or (not "PLMAF" in site.INFO):
		#	nnowsaf += 1
		#	continue

		wsaf = np.ma.masked_values(site.format("WSAF"), -1)
		plmaf = min(np.average(wsaf), 1 - np.average(wsaf))
		#plmaf = site.INFO["PLMAF"]
		H_px = 1 - (plmaf**2 + (1 - plmaf)**2)
		H_sx = 1 - (np.power(wsaf, 2) + np.power((1 - wsaf), 2))
		H_p.append(H_px)
		H_s.append(H_sx)

	log.info("Done reading VCF.".format(ii))
	log.info("\t{: >12d} biallelic in called genotypes".format(ii-nmulti))
	log.info("\t{: >12d} non-polymorphic in called genotypes".format(nnonpoly))
	log.info("\t{: >12d} multiallelic".format(nmulti))
	log.info("\t{: >12d} without WSAF,PLMAF annotations".format(nnowsaf))
	log.info("\t------------")
	log.info("\t{: >12d} total sites\n".format(ii))

	log.info("Calculating F_ws ...")
	if args.jackknife is not None:
		log.info(" [ standard errors estimated by block jackknife in blocks of size ~= {} ]".format(args.jackknife))
	else:
		log.info(" [ asymptotic standard errors ]")
	## matrices of H_px and H_sx
	H_p = np.array(H_p, dtype = np.float).reshape(len(H_p), 1)
	H_s = np.ma.asanyarray(H_s)[:,:,0]
	#print(np.sum(H_s.mask))
	#print(H_s.shape)

	def do_fws(Z, do_se = False):
		y = Z[:,0,None] # None stops numpy dropping a dimension here
		X = Z[:,1:]
		beta, sse, rank, sv = np.linalg.lstsq(y, X, rcond = -1) # `rcond` passed explicitly to avoid FutureWarning
		if do_se:
			sigma2 = np.sum((y - np.dot(X, beta))**2) / (X.shape[0] - 1)
			C = sigma2 * np.linalg.pinv(np.dot(X.T, X)) # covariance matrix
			se = np.sqrt(np.diag(C)) # standard error
		F_ws = 1 - beta
		if do_se:
			return F_ws[0,0], se[0]
		else:
			return F_ws[0,0]

	## do no-intercept regression of H_sx on H_px
	for ii,iid in enumerate(vcf.samples):
		H_sx = H_s[:,ii]
		X = H_sx[ ~H_sx.mask ]
		X = X.reshape(len(X), 1)
		y = H_p[ ~H_sx.mask ]
		Z = np.hstack((y,X))
		if Z.shape[0] > 0:
			if args.jackknife is not None:
				se, _ = block_jackknife(Z, do_fws, args.jackknife)
				F_ws = do_fws(Z)
			else:
				F_ws, se = do_fws(Z, True)
			print(iid, F_ws, se, np.sum(~H_sx.mask), sep = "\t")
		else:
			print(iid, np.nan, np.nan, 0, sep = "\t")

	log.info("Done.\n")


def pca_wsaf(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## decide which samples to use
	if args.samples_file:
		keep_samples = []
		for line in args.samples_file:
			if line.startswith("#"):
				continue
			else:
				iid = line.strip().split().pop(0)
				if not iid in vcf.samples:
					log.warning("Sample '{}' in populations file but not in VCF; skpping it.".format(iid))
					continue
				keep_samples.append(iid)
		log.info("Read {} samples from file <{}>".format(len(keep_samples), args.samples_file.name))
	elif args.samples is not None and len(args.samples):
		keep_samples = args.samples
		log.info("Read {} samples from command line".format(len(keep_samples)))
	else:
		keep_samples = None

	if keep_samples is not None:
		keep_samples = set(vcf.samples) & set(keep_samples)
		keep_samples = list(keep_samples)
		nsamples = len(keep_samples)
		log.info("Retained {} distinct samples that were verified in VCF header.".format(nsamples))
	else:
		keep_samples = list(vcf.samples)
		nsamples = len(vcf.samples)
		log.info("Retained all {} samples in VCF.".format(nsamples))

	CHUNK_SIZE = args.chunk_size
	ii = 0
	nkept = 0
	W = np.zeros((CHUNK_SIZE, nsamples), dtype = np.float)
	log.info("Reading genotype data in chunks of size {} ...".format(CHUNK_SIZE))

	sample_idx = [ vcf.samples.index(_) for _ in keep_samples ]
	for site in vcf:

		ii += 1
		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))
		if ii > ns and ns > -1:
			ii -= 1
			break

		if site.INFO["PLMAF"] < args.min_plmaf:
			continue

		nkept += 1
		if nkept >= W.shape[0]:
			log.debug("Increasing array allocation ... ")
			W = np.append( W, np.zeros((CHUNK_SIZE, nsamples), dtype = np.float), axis = 0 )

		## get WSAFs from FORMAT field
		w = site.format("WSAF")
		w = w[sample_idx,0]

		## impute PLAF for missing values
		plaf = np.average(w[ w != -1 ])
		ww = w[:]
		ww[ ww == -1 ] = plaf
		W[nkept,:] = ww

	## remove low-variance sites that make bad things happen in SVD
	W = W[:nkept,:]
	sigma = np.var(W, 1)
	W = W[ (sigma > 1.0e-6),:]
	log.info("Loaded genotypes at {} sites with PLMAF >= {}".format(nkept, args.min_plmaf))
	log.info("Retained {} sites with non-negligible variance for PCA.".format(W.shape[0]))

	## do the PCA using scikit-allel tools
	log.info("Performing PCA ...")
	pca_fn = pca if not args.randomized else randomized_pca
	coords, model = pca_fn(W, n_components = args.ncomponents, scaler = "patterson")
	log.info("Done with PCA.")

	## write rotations and normalized eigenvalues
	log.info("Writing results to <{0}.pca> and <{0}.pca.ev> (sample coordinates and variance explained, respectively.)".format(args.out))
	with open(args.out + ".pca", "w") as pcs_file:
		for ii in range(0, coords.shape[0]):
			print(vcf.samples[ii], *coords[ii,:], sep = "\t", file = pcs_file)
	with open(args.out + ".pca.ev", "w") as evs_file:
		print("\n".join([ str(_) for _ in model.explained_variance_ratio_ ]), file = evs_file)

	## all done
	log.info("Done.\n")


def calc_weighted_dist(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)

	CHUNK_SIZE = args.chunk_size
	def give_chunk():
		return np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.float)

	wsaf = give_chunk()
	wsaf_imputed = give_chunk()

	## loop on VCF and add take sample depths
	ii = 0
	logger.info("Reading genotype data in chunks of size {} ...".format(CHUNK_SIZE))
	for site in vcf:

		ii += 1
		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		if ii >= wsaf.shape[0]:
			log.debug("Increasing array allocation ... ")
			wsaf = np.append( wsaf, give_chunk(), axis = 0 )
			wsaf_imputed = np.append( wsaf_imputed, give_chunk(), axis = 0 )

		## get WSAFs from FORMAT field
		this_wsaf = site.format("WSAF")
		wsaf[ii,:] = this_wsaf[:,0]

		## impute PLAF for missing values
		plaf = np.average(this_wsaf[ this_wsaf != -1 ])
		ww = this_wsaf[:,0]
		ww[ ww == -1 ] = plaf
		wsaf_imputed[ii,:] = ww

	wsaf = wsaf[ :ii, : ]
	wsaf_imputed = wsaf_imputed[ :ii, : ]
	log.info("Dimensions of full genotype matrix: {}".format(wsaf.shape))
	log.info("Calculating weighting factors following Amato et al (PMC4786412) ...")
	r_sq = np.corrcoef(wsaf_imputed) ** 2
	r_sq[ np.isnan(r_sq) ] = 0
	r_sq -= np.eye(r_sq.shape[0])
	r_sq[ r_sq < kwargs["rsq_cutoff"] ] = 0
	w_i = 1 / (1 + np.sum(r_sq, 1))

	log.info("Calculating pairwise distances using WSAFs ...")
	combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
	log.info("\t[{} combinations]".format(len(combos)))
	formatter = "{:0." + str(args.digits) + "f}"
	#D = np.zeros( (len(vcf.samples), len(vcf.samples)), dtype = np.float )
	ii = 0
	for sa,sb in combos:
		ii += 1
		fa = np.ma.masked_values(wsaf[:,sa], -1)
		fb = np.ma.masked_values(wsaf[:,sb], -1)
		d_i = (fa*(1-fb) + (1-fa)*fb)*w_i
		d = np.average(d_i)
		#D[ sa,sb ] = np.average(d*w_i)
		nz = np.sum(~d_i.mask)
		if not ii % 100:
			log.info("\t... {} pairs done ...".format(ii))
		print(vcf.samples[sa], vcf.samples[sb], nz, formatter.format(d), sep = "\t")

	log.info("Done.\n")

def calc_ibs_dist(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)

	CHUNK_SIZE = args.chunk_size
	def give_chunk():
		return np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.float)

	geno = give_chunk()
	geno_imputed = give_chunk()

	## loop on VCF and add take sample depths
	ii = 0
	nused = 0
	logger.info("Reading genotype data in chunks of size {} ...".format(CHUNK_SIZE))
	for site in vcf:

		ii += 1
		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites [{} kept] ...".format(ii-1, nused))

		if ii > ns and ns > -1:
			ii -= 1
			break

		this_geno = site.gt_types/site.ploidy
		ac = np.sum(np.ma.masked_values(this_geno, 3))
		an = np.sum(this_geno == 3)
		if ac == 2*an or ac == 0:
			continue
		nused += 1

		if nused >= geno.shape[0]:
			log.debug("Increasing array allocation ... ")
			geno = np.append( geno, give_chunk(), axis = 0 )
			geno_imputed = np.append( geno_imputed, give_chunk(), axis = 0 )

		## get genotype calls as number of alt alleles
		this_geno = site.gt_types/site.ploidy
		geno[nused,:] = this_geno[:]

		## impute PLAF for missing values
		plaf = np.average(this_geno[ this_geno != 3 ])
		ww = this_geno[:]
		ww[ ww == 3 ] = plaf
		geno_imputed[nused,:] = ww

	geno = geno[ :nused, : ]
	geno_imputed = geno_imputed[ :nused, : ]
	log.info("Dimensions of full genotype matrix: {}".format(geno_imputed.shape))

	log.info("Calculating pairwise distances ...")
	D = squareform( pairwise_distance(geno_imputed, "hamming") )
	D = 1 - D
	formatter = "{:0." + str(args.digits) + "f}"

	if not args.flatten:
		print(".", *vcf.samples, sep = "\t")
		for jj,iid in enumerate(vcf.samples):
			rounded = [ formatter.format(_) for _ in D[jj,:] ]
			print(vcf.samples[jj], *rounded, sep = "\t")
	else:
		combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
		log.info("\t[{} combinations]".format(len(combos)))
		ii = 0
		for sa,sb in combos:
			ii += 1
			fa = np.ma.masked_values(geno[:,sa], 3)
			fb = np.ma.masked_values(geno[:,sb], 3)
			nz = np.sum(np.logical_and(geno[:,sa] != 3, geno[:,sb] != 3))
			d = D[sa,sb]
			if not ii % 100:
				log.info("\t... {} pairs done ...".format(ii))
			print(vcf.samples[sa], vcf.samples[sb], nz, formatter.format(d), sep = "\t")

	log.info("Done.\n")

def summarise_dp(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)

	CHUNK_SIZE = args.chunk_size
	sample_dp = np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.int)

	## loop on VCF and add take sample depths
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1
		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))
		if ii > ns and ns > -1:
			ii -= 1
			break

		if ii >= sample_dp.shape[0]:
			log.debug("Increasing array allocation ... ")
			sample_dp = np.append( sample_dp, np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.int), axis = 0 )

		this_dp = site.format("DP").T[0:]
		this_dp[ this_dp < 0 ] = 0
		sample_dp[ii,:] = this_dp

	mu = np.mean(sample_dp[:ii,:], 0)
	sigma = np.std(sample_dp[:ii,:], 0)
	med = np.percentile(sample_dp[:ii,:], 50, 0)

	for iid, _mu, _med, _sigma in zip(vcf.samples, mu, med, sigma):
		print(iid, "{:.1f}".format(_mu), "{:.1f}".format(_med), "{:.3f}".format(_sigma), sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))


def summarise_flt(vcf, log, ns, **kwargs):

	## loop on VCF and add take sample depths
	ii = 0
	what_mode = None
	if args.samples:
		counter = (
			{ iid: defaultdict(int) for iid in vcf.samples },
			{ iid: defaultdict(int) for iid in vcf.samples }
		)
		what_mode = "SAMPLES"
	elif args.sites:
		counter = defaultdict(int)
		what_mode = "SITES"
	else:
		log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	log.info("Talling filters by {}".format(what_mode))
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		site_filt = "PASS" if site.FILTER is None else site.FILTER
		if what_mode == "SITES":
			counter[site_filt] += 1
		elif what_mode == "SAMPLES":
			jj = 0 if site_filt == "PASS" else 1
			if "FT" in site.FORMAT:
				for iid, geno_filt in zip(vcf.samples, site.format("FT")):
					counter[jj][iid][geno_filt] += 1
			else:
				for iid in vcf.samples:
					counter[jj][iid]["PASS"] += 1

	#print(counter)
	if what_mode == "SITES":

		for filt,count in counter.items():
			print(filt, count, sep = "\t")

	elif what_mode == "SAMPLES":

		for iid in vcf.samples:
			for jj in (0,1):
				flag = "PASS" if ii else "FAIL"
				for filt,count in counter[jj][iid].items():
					print(iid, flag, filt, count, sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))


def calc_missingness(vcf, log, ns, **kwargs):

	## loop on VCF and count missing calls
	ii = 0
	what_mode = None
	if args.samples:
		counts = np.zeros(len(vcf.samples), dtype = np.int)
		what_mode = "SAMPLES"
	elif args.sites:
		what_mode = "SITES"
	else:
		log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	log.info("Talling missingness by {}".format(what_mode))
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		if what_mode == "SITES":
			nmiss = np.sum(site.gt_types == 3)
			ntot = len(vcf.samples)
			print(site.CHROM, site.POS, site.ID, site.REF, ",".join(site.ALT), nmiss, nmiss/ntot, sep = "\t")
		elif what_mode == "SAMPLES":
			counts += (site.gt_types == 3)

	#print(counter)
	if what_mode == "SITES":
		pass
	elif what_mode == "SAMPLES":
		for iid,nmiss in zip(vcf.samples, counts):
			print(iid, nmiss, nmiss/ii, sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))

def polarize_anc_vs_der(vcf, log, ns, **kwargs):

	## connect to ancestral genome file
	genome = Fasta(args.fasta)
	log.info("Ancestral genome: <{}>".format(args.fasta))

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": "AA", "Description": "Ancestral allele inferred from outgroup", "Number": 1, "Type": "String" })
	sys.stdout.write(vcf.raw_header)

	def define_anc_allele(alleles, og):
		ref = alleles[0].upper()
		alt = [ x.upper() for x in alleles[1:] ]
		new_alleles = [ref] + alt
		og = og.upper()
		anc = "X"*len(ref)
		for ii,a in enumerate(new_alleles):
			if a == og:
				anc = alleles[ii]
				break
		return anc

	## loop on VCF and add 'AA' tag
	nanc = 0
	nder = 0
	nmissing = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		start = site.POS - 1
		end = start + len(site.REF)
		alleles = [site.REF] + site.ALT
		og = str(genome[site.CHROM][start:end])
		anc = define_anc_allele(alleles, og)
		#print(og, alleles, anc, file = sys.stderr)

		if anc == "X":
			nmissing += 1
		elif anc == alleles[0]:
			nder += 1
		elif anc in alleles[1:]:
			nanc += 1

		site.INFO["AA"] = anc
		sys.stdout.write(str(site))

	log.info("Done.".format(ii))
	log.info("\t{: >12d} had undefined ancestral allele".format(nmissing))
	log.info("\t{: >12d} have REF = ancestral".format(nder))
	log.info("\t{: >12d} have REF = derived".format(nanc))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

def transition_or_transversion(vcf, log, ns, **kwargs):

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": "Transversion", "Description": "Is this site a transition (0), transversion (1), or undefined (-1)?", "Number": 1, "Type": "Integer" })
	vcf.add_info_to_header({ "ID": "StrongWeak", "Description": "H-bond strength of ancestral and derived states (w=weak, s=strong)", "Number": 1, "Type": "String" })
	vcf.add_info_to_header({ "ID": "BGC", "Description": "Is this site a candidate for GC-biased gene coversion? 0=No, 1=Yes, -1=undefined", "Number": 1, "Type": "Integer" })
	sys.stdout.write(vcf.raw_header)

	def ti_or_tv(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		if subst in ["A>G","C>T","G>A","T>C"]:
			return 0
		else:
			return 1

	def is_bgc(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		if anc in ["A","T"] and der in ["G","C"]:
			return 1
		else:
			return 0

	def strong_or_weak(anc, der):
		anc = anc.upper()
		der = der.upper()
		subst = anc + ">" + der
		strength = { "A": "w", "C": "s", "G": "s", "T": "w" }
		return strength[anc] + strength[der]

	def good_alleles(alleles):
		alleles = "".join(alleles).upper()
		return all(_ in "ACGT" for _ in alleles)

	## loop on VCF
	nti, ntv, nbgc = 0, 0, 0
	nnotsnv = 0
	nnoanc = 0
	#nbadprefix = 0
	nbadallele = 0
	nmulti = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		if not (site.is_snp and not site.is_indel):
			nnotsnv += 1
			continue

		if len(site.ALT) > 1:
			nmulti += 1
			continue

		if not good_alleles([site.REF] + site.ALT):
			nbadallele += 1
			continue

		## absent or bogus ancestral allele?
		anc = "X"
		try:
			anc = site.INFO["AA"]
			#log.info("Ancestral allele is '{}'".format(anc))
		except KeyError as e:
			nnoanc += 1
			continue

		if anc == "X":
			nnoanc += 1
			continue

		if anc == site.REF:
			der = site.ALT[0]
		elif anc == site.ALT[0]:
			der = site.REF
		else:
			nnoanc += 1
			continue

		titv = ti_or_tv(anc, der)
		if titv == 0:
			nti += 1
		elif titv == 1:
			ntv += 1
		sw = strong_or_weak(anc, der)
		bgc = is_bgc(anc, der)
		if bgc == 1:
			nbgc += 1
		site.INFO["Transversion"] = titv
		site.INFO["StrongWeak"] = sw
		site.INFO["BGC"] = bgc

		sys.stdout.write(str(site))

	log.info("Done.".format(ii))
	log.info("\t{: >12d} not SNVs".format(nnotsnv))
	log.info("\t{: >12d} with >2 alleles".format(nmulti))
	log.info("\t{: >12d} with bad allele(s)".format(nbadallele))
	log.info("\t{: >12d} transitions".format(nti))
	log.info("\t{: >12d} transversions".format(ntv))
	log.info("\t{: >12d} gBGC candidates".format(nbgc))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))
	log.info("\t[ TiTv = {:0.3f} ]".format(nti/ntv))

def count_derived(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)
	if not (args.reads or args.calls) or (args.reads and args.calls):
		log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	log.info("Counting derived alleles at level of {}".format(how_count))

	## initialize counters
	what_type = np.int if args.calls else np.float
	counts = np.zeros(len(vcf.samples), dtype = what_type)
	nsites = np.zeros(len(vcf.samples), dtype = np.int)

	## loop on VCF
	ploidy = None
	nambig = 0
	npol = 0
	nmulti = 0
	nuncountable = 0
	nused = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		## absent or bogus ancestral allele?
		anc = "X"
		try:
			anc = site.INFO["AA"]
			#log.info("Ancestral allele is '{}'".format(anc))
		except KeyError as e:
			nambig += 1
			continue

		if anc == "X":
			nambig += 1
			continue

		npol += 1

		## not biallelic?
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		## sniff ploidy
		if ploidy is None:
			pieces = resplit(r"[/|]", site.gt_bases[0])
			ploidy = len(pieces)
			log.info("Detected ploidy = {} in this VCF".format(ploidy))

		## biallelic, has non-bogus ancestral allele
		if args.reads:
			if not "WSAF" in site.FORMAT:
				nuncountable += 1
				continue
			else:
				gt = site.format("WSAF")[:,0]
				if not site.REF == anc:
					gt = 1 - gt
				nsites[ gt > -1 ] += 1
				counts[ gt != -1 ] += gt[ gt != -1 ]
				nused += 1
		else:
			gt = site.gt_types
			nsites[ gt < 3 ] += 1
			counts[ gt < 3 ] += gt[ gt < 3 ]
			nused += 1

	if args.calls:
		counts = counts/ploidy
	for jj,iid in enumerate(vcf.samples):
		print(iid, counts[jj], nsites[jj], counts[jj]/nsites[jj], ploidy, sep = "\t")

	log.info("Done.")
	log.info("\t{: >12d} had usable data".format(nused))
	log.info("\t{: >12d} had undefined ancestral allele".format(nambig))
	log.info("\t{: >12d} had >2 alleles".format(nmulti))
	if args.reads:
		log.info("\t{: >12d} had no WSAF annotation".format(nuncountable))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

def flag_private(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## identify private alleles by hard calls or read counts?
	if not (args.reads or args.calls) or (args.reads and args.calls):
		log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	what_count = "READS" if args.reads else "CHROMOSOMES"
	log.info("Defining private alleles at level of {}".format(how_count))
	log.info("Cutoff for allele presence: {} supporting {}".format(args.min_count, what_count))

	## decide which samples to use
	if args.samples_file:
		keep_samples = []
		for line in args.samples_file:
			if line.startswith("#"):
				continue
			else:
				iid = line.strip().split().pop(0)
				if not iid in vcf.samples:
					log.warning("Sample '{}' in populations file but not in VCF; skpping it.".format(iid))
					continue
				keep_samples.append(iid)
		log.info("Read {} samples from file <{}>".format(len(keep_samples), args.samples_file.name))
	elif len(args.samples):
		keep_samples = []
		for iid in args.samples:
			keep_samples.extend(resplit("[,;|]", iid))
		log.info("Read {} samples from command line".format(len(keep_samples)))
	else:
		log.error("Must specify at least one focal sample.")
		sys.exit(1)

	keep_samples = set(vcf.samples) & set(keep_samples)
	keep_samples = list(keep_samples)
	nsamples = len(keep_samples)
	log.info("Retained {} distinct samples that were verified in VCF header.".format(nsamples))

	## get sample positions in input VCF
	focal_idx = [ vcf.samples.index(_) for _ in keep_samples ]
	nonfocal_idx = [ _ for _,iid in enumerate(vcf.samples) if not iid in keep_samples ]

	def is_private(counts):
		if args.reads:
			counts[ counts < 0 ] = 0
		else:
			counts[ counts == 3 ] = 0
		ac_in = np.sum(counts[focal_idx])
		ac_out = np.sum(counts[nonfocal_idx])
		return ac_in >= args.min_count and ac_out < args.min_count

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": args.flag, "Description": "Private allele in specified subset", "Type": "Flag", "Number": "1"})
	sys.stdout.write(vcf.raw_header)

	nused = 0
	nnodp = 0
	nmulti = 0
	npriv = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		## not biallelic?
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		if args.reads:
			if not "AD" in site.FORMAT:
				nnodp += 1
				continue
			else:
				nused += 1
			counts = site.format("AD")[:,1]
		else:
			nused += 1
			counts = site.gt_types[:]

		if is_private(counts):
			npriv += 1
			site.INFO[args.flag] = True

		sys.stdout.write(str(site))

	log.info("Done.")
	log.info("\t{: >12d} had usable data".format(nused))
	log.info("\t[{: >12d} flagged private to {} samples]".format(npriv, nsamples))
	if args.reads:
		log.info("\t{: >12d} had no AD annotation".format(nnodp))
	log.info("\t{: >12d} had >2 alleles".format(nmulti))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

def calc_sfs(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## set RNG seed
	if args.seed >= 0:
		log.info("RNG seed := {}".format(args.seed))
		np.random.seed(args.seed)
	else:
		log.info("RNG seed := auto [!! NON-REPRODUCIBLE !!]")

	## calculate SFS by hard calls or read counts?
	if not (args.reads or args.calls) or (args.reads and args.calls):
		log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READ COUNTS" if args.reads else "GENOTYPE CALLS"
	log.info("Calculating uSFS from {}".format(how_count))

	if not args.pops:
		log.info("Treating all samples in VCF as coming from one population.")
		pops = defaultdict(list)
		pops["pop1"].extend( list(range(0, len(vcf.samples))) )
	else:
		log.info("Reading population assignments from <{}>".format(args.pops.name))
		## decide which samples to use
		pops = defaultdict(list)
		seen = defaultdict(int)
		for line in args.pops:
			if line.startswith("#"):
				continue
			pieces = line.strip().split()
			# not an iid,pop tuple? skip
			if len(pieces) < 2:
				continue
			iid, pop = pieces[:2]
			# sample not in VCF? skip
			if not iid in vcf.samples:
				log.warning("Sample '{}' in populations file but not in VCF; skpping it.".format(iid))
				continue
			# already seen this sample? assume first assignment was right
			if iid in seen:
				continue
			else:
				seen[iid] += 1
			# finally, add sample to population
			pops[pop].append( vcf.samples.index(iid) )

	pop_order = sorted(pops, key = lambda k: -1*len(pops[k]))
	ssz = [ len(pops[_]) for _ in pop_order ]
	dims = tuple([ _ + 1 for _ in ssz ])

	log.info("Populations and their sizes:")
	for ii,pop in enumerate(pops.keys()):
		log.info("\t{}  {: >4d}".format(pop, ssz[ii]))

	## initialize SFS
	sfs = np.zeros(dims, dtype = np.int)
	log.info("SFS will have these dimensions: {}".format(dims))

	## loop on VCF
	nused = 0
	nnodp = 0
	nmulti = 0
	nnoanc = 0
	ii = 0
	ploidy = None
	log.info("Processing VCF file ...")
	for site in vcf:

		## sniff ploidy
		if ploidy is None:
			pieces = resplit(r"[/|]", site.gt_bases[0])
			ploidy = len(pieces)
			log.info("Detected ploidy = {} in this VCF".format(ploidy))

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		## not biallelic?
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		## no ancestral allele?
		anc = "X"
		try:
			anc = site.INFO["AA"]
			#log.info("Ancestral allele is '{}'".format(anc))
		except KeyError as e:
			nnoanc += 1
			continue

		if anc == "X":
			if args.ref_is_ancestral:
				anc = site.REF
			else:
				nnoanc += 1
				continue

		if args.reads:
			if not "WSAF" in site.FORMAT:
				nnodp += 1
				continue
			else:
				nused += 1
			counts = site.format("WSAF")[:,0]
			sampled = np.zeros(len(ssz), dtype = np.int)
			for jj,pop in enumerate(pop_order):
				these_counts = counts[ pops[pop] ]
				if np.all(these_counts == -1):
					continue
				good_counts = these_counts[ these_counts > -1 ]
				maf = np.average(good_counts)
				these_counts[ these_counts == -1 ] = maf
				if not anc == site.REF:
					these_counts = 1 - these_counts
				sampled[jj] = np.sum(np.random.binomial(1, these_counts))
			sampled = tuple(sampled)
		else:
			nused += 1
			counts = site.gt_types[:]
			sampled = np.zeros(len(ssz), dtype = np.int)
			for jj,pop in enumerate(pop_order):
				these_counts = counts[ pops[pop] ]
				omit = (these_counts == 3)
				if np.all(omit):
					continue
				these_counts = these_counts/ploidy
				good_counts = these_counts[ ~omit ]
				maf = np.average(good_counts)
				these_counts[ omit ] = maf
				if not anc == site.REF:
					these_counts = 1 - these_counts
				sampled[jj] = np.sum(np.random.binomial(1, these_counts))
			sampled = tuple(sampled)

		## finally, update the sfs
		sfs[ sampled ] += 1

	log.info("Done.")
	log.info("\t{: >12d} had usable data".format(nused))
	log.info("\t{: >12d} had undefined ancestral allele".format(nnoanc))
	if args.reads:
		log.info("\t{: >12d} had no WSAF annotation".format(nnodp))
	log.info("\t{: >12d} had >2 alleles".format(nmulti))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

	print("#pops={}".format(",".join(pop_order)))
	print("#dims={}".format(",".join([ str(_) for _ in dims ])))
	print("#nsites={}".format(nused))
	print(*sfs.flatten(), sep = " ")

def thin_sites(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## write outgoing VCF header
	sys.stdout.write(str(vcf.raw_header))

	ii = 0
	nkept = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			break

		if not ii % args.every:
			nkept += 1
			sys.stdout.write(str(site))

	log.info("Done.")
	log.info("\t{: >12d} kept ({:0.3f}%)".format(nkept, 100*nkept/ii))
	log.info("\t{: >12d} discarded".format(ii-nkept))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

def prune_sites_by_ld(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## write outgoing VCF header
	sys.stdout.write(str(vcf.raw_header))

	CHUNK_SIZE = args.chunk_size
	def give_chunk():
		return np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.float)

	log.info("Performing LD pruning to r^2 <= {:0.3f} in windows of {} sites (stepping by {})".format(args.r_squared, args.window, args.step))
	## straight from <http://alimanfoo.github.io/2015/09/28/fast-pca.html>
	def ld_prune(gn, size, step, threshold = 0.1, n_iter = 1):
		idx = np.arange(gn.shape[0])
		for i in range(n_iter):
			loc_unlinked = locate_unlinked(gn, size=size, step=step, threshold=threshold)
			n = np.count_nonzero(loc_unlinked)
			n_remove = gn.shape[0] - n
			log.info("\t ... iteration {}: retaining {} of {} sites".format(i, n, gn.shape[0]))
			gn = gn[ loc_unlinked,... ]
			idx = idx[ loc_unlinked ]
		return idx

	## read genotypes to matrix, one chromosome at a time
	ii = 0
	nkept = 0
	ntot = 0
	log.info("Processing VCF file ...")
	last_chrom = None
	pos = []
	geno = give_chunk()

	## while instead of for-loop to avoid gymnasitcs at ends of chrosomes
	# while True:
	#
	# 	# consume next site
	# 	try:
	# 		site = next(vcf)
	# 	except StopIteration as e:
	# 		break

	for site in vcf:
		# increment counters
		ii += 1
		nkept += 1

		if (ii-1) and not (ii-1) % 1000:
			log.info("\t... processed {} sites ...".format(ii-1))

		if ii > ns and ns > -1:
			ii -= 1
			nkept -= 1
			break

		# increase array allocation, if needed
		if nkept >= geno.shape[0]:
			log.debug("Increasing array allocation {} --> {} ... ".format(geno.shape[0], geno.shape[0]+CHUNK_SIZE))
			geno = np.append( geno, give_chunk(), axis = 0 )

		if last_chrom and last_chrom != site.CHROM:
			# started new chromosome; calculate LD, reset containers and keep going
			log.info("Pruning sites on chromosome '{}'".format(last_chrom))
			keep = ld_prune(geno[:(nkept-1),:], args.window, args.step, args.r_squared, args.niter)
			log.debug("{} {} {}".format(nkept-1, np.max(keep), len(pos)))
			for jj in keep:
				ntot += 1
				sys.stdout.write(str(pos[jj]))
			geno = give_chunk()
			pos = []
			nkept = 0
			last_chrom = str(site.CHROM)
			continue

		last_chrom = str(site.CHROM)
		geno[nkept,:] = site.gt_types/2 * site.ploidy
		geno[nkept, site.gt_types == 3 ] = np.nan
		pos.append(str(site))

	## done with file; flush buffers
	log.info("Pruning sites on chromosome '{}'".format(last_chrom))
	keep = ld_prune(geno[:nkept,:], args.window, args.step, args.r_squared, args.niter)
	log.debug("{} {} {}".format(nkept, np.max(keep), len(pos)))
	for jj in keep:
		ntot += 1
		sys.stdout.write(str(pos[jj]))

	log.info("Done.")
	log.info("\t{: >12d} sites in approx linkage equilibrium ({:0.3f}%)".format(ntot, 100*ntot/ii))
	log.info("\t{: >12d} pruned for LD".format(ii-ntot))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))


## root-level argument parser
def root_help_message(name = None):
	return r"""
	vcfdo <command> [options]

	Commands:

	  --- Basic statistics
	      missing           rates of missing calls by site or by sample
	      depthsummary      quantiles of read depth (at called sites) per sample
	      filtersummary     tallies of filter status by site or by sample

	  --- Downsampling
	      thin              emit every nth site, possibly with random starting point
	      prune             greedy LD-pruning in sliding windows

	  --- Ancestral alleles
	      polarize          define ancestral alleles based on outgroup sequence
	      titv              annotate sites as transitions or transversions; flag gBGC candidates
	      derived           count derived alleles per sample, using either read counts or hard calls

	  --- Allele frequencies
	      wsaf              calculate within-sample allele frequency (WSAF) and related quantities from read counts
	      fws               estimate pseudo-inbreeding coefficient F_ws ("within-sample relatedness")
	      private           annotate sites where non-ref allele is only found in certain subset of samples
	      sfs               approximate the n-dimensional unfolded SFS by sampling

	  --- Relatedness/ordination
	      dist              calculate LD-weighted pairwise distances from within-sample allele frequencies
	      ibs               calculate simple identity-by-state (IBS) matrix from hard calls at polymorphic sites only
	      pca               perform PCA using within-sample allele frequencies instead of hard calls

	Notes:
	* All commands that take VCF as input or output work on pipes. For basic filtering operations,
	it is recommended to use `bcftools` and pipe to this tool.
	* Input can be in any format that `htslib` can handle -- VCF, bgzipped VCF, BCF, etc.
	* Commands which modify the VCF will add an entry to the header recording the command line used,
	the current working directory and a timestamp.
	"""

parser = argparse.ArgumentParser(
	description = "Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.",
	usage = root_help_message(),
	add_help = False)
subparsers = parser.add_subparsers()
parser.add_argument("-i","--infile",
					default = "/dev/stdin",
					help = "VCF file, possibly gzipped [default: stdin]")
parser.add_argument("-n","--nsites", type = int,
					help = "stop after processing this many sites [default: no limit]")
parser.add_argument("--chunk-size", type = int,
					default = 2000,
					help = "when loading genotypes into memory, process this many sites per chunk [default: %(default)d]")
parser.add_argument("--seed", type = int,
					default = 1,
					help = "random number seed for reproducibility, values < 0 use auto seed; only affects stuff involving sampling [default: %(default)d]")
parser.add_argument("-q","--quiet", action = "store_true",
					help = "see fewer logging messages")
parser.add_argument("-v","--verbose", action = "store_true",
					help = "see more logging messages")

## calculate WSAF, PLAF, PLMAF
wsaf_parser = subparsers.add_parser("wsaf",
	description = "Calculate within-sample allele frequencies (WSAF) from read counts, and their population-level equivalents.",
	parents = [parser])
wsaf_parser.add_argument("--set-filter", type = float,
						default = None,
						help = "apply genotype-level filter for samples with WSAF outside specified bounds [default: don't do this]")
wsaf_parser.set_defaults(func = calc_wsaf, command = "wsaf")

## calculate F_ws from WSAF
fws_parser = subparsers.add_parser("fws",
	description = "Calculate within-sample allele 'inbreeding'-like coefficient F_ws, using precomputed WSAFs. Optionally get standard errors by block jackknife.",
	parents = [parser])
fws_parser.add_argument("-j","--jackknife", type = int,
					default = None,
					help = "estimate standard errors by block jackknife with this block size (number of sites) [default: use asymptotics]")
fws_parser.set_defaults(func = calc_fws, command = "fws")

## summarise depth per sample per called site
dp_parser = subparsers.add_parser("depthsummary",
	description = "Summarise depth (FORMAT:DP) per site per sample, splitting by filter status of sites.",
	parents = [parser])
dp_parser.set_defaults(func = summarise_dp, command = "depthsummary")

## summarise filters at site or individual level
flt_parser = subparsers.add_parser("filtersummary",
	description = "Tally filter status of sites or samples.",
	parents = [parser])
flt_parser.add_argument("--sites", action = "store_true",
						help = "tally filter status of sites")
flt_parser.add_argument("--samples", action = "store_true",
						help = "tally filter status of per-sample genotypes")
flt_parser.set_defaults(func = summarise_flt, command = "filtersummary")

## summarise filters at site or individual level
miss_parser = subparsers.add_parser("missing",
	description = "Tally missing calls per site or per sample.",
	parents = [parser])
miss_parser.add_argument("--sites", action = "store_true",
						help = "tally missingness per site")
miss_parser.add_argument("--samples", action = "store_true",
						help = "tally missingness per sample, across all sites")
miss_parser.set_defaults(func = calc_missingness, command = "missing")

## do PCA on WSAFs
pca_parser = subparsers.add_parser("pca",
	description = "Perform PCA on with-sample allele frequencies instead of genotype calls.",
	parents = [parser])
pca_parser.add_argument("-o","--out",
					default = "wsaf_pca",
					help = "prefix for a pair of output files, *.pca and *.pca.ev [default: %(default)s]")
pca_parser.add_argument("-s","--samples", nargs = "+",
					help = "restrict analysis to these samples, whitespace-separated [default: use all samples]")
pca_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "restrict analysis to samples in this file, one per line [default: use all samples]")
pca_parser.add_argument("-f","--min-plmaf", type = float,
					default = 0.0,
					help = "limit to sites with PMLAF > {min_plmaf} [default: %(default)f]")
pca_parser.add_argument("-K","--ncomponents", type = int,
					default = 10,
					help = "calculate the first K PCs if using randomized SVD algorithm [default: %(default)d]")
pca_parser.add_argument("--randomized", action = "store_true",
					help = "use fast randomized SVD algorithm")
pca_parser.set_defaults(func = pca_wsaf, command = "pca")

## genetic distance from WSAFs, weighted for LD
dist_parser = subparsers.add_parser("dist",
	description = "Calculate LD-weighted genetic distances from within-sample allele frequencies according to method of Amato et al. (PMC4786412).",
	parents = [parser])
dist_parser.add_argument("-d","--digits", type = int,
					default = 5,
					help = "report distance to this many decimal places [default: %(default)d]")
dist_parser.add_argument("-r","--rsq-cutoff", type = float,
					default = 0.10,
					help = "ignore site pairs with r^2 < cutoff when assigning weights [default: %(default)f]")
dist_parser.set_defaults(func = calc_weighted_dist, command = "dist")

## quick identity-by-state on hard genotypes
ibs_parser = subparsers.add_parser("ibs",
	description = "Calculate identity-by-state (IBS) matrix from hard genotype calls, without weights (but only polymorphic sites).",
	parents = [parser])
ibs_parser.add_argument("-d","--digits", type = int,
					default = 3,
					help = "report IBS to this many decimal places [default: %(default)d]")
ibs_parser.add_argument("--flatten", action = "store_true",
					help = "report one pair of individuals per line, rather than square matrix")
ibs_parser.set_defaults(func = calc_ibs_dist, command = "ibs")

## polarize alleles using ancestral/outgroup genome in fasta format
anc_parser = subparsers.add_parser("polarize",
	description = "Polarize alleles as ancestral vs derived, based on outgroup sequence",
	parents = [parser])
anc_parser.add_argument("-f","--fasta",
					required = True,
					help = "ancestral genome fasta file, **in same coordinates as reference**")
anc_parser.set_defaults(func = polarize_anc_vs_der, command = "polarize")

## annotate sites as transition or transversion, gBGC or not, and biochemical "strength"
anc_parser = subparsers.add_parser("titv",
	description = "Annotate variants as transition or transversion; flag gBGC candidates",
	parents = [parser])
anc_parser.set_defaults(func = transition_or_transversion, command = "titv")

## count number of derived calls per sample
anc_parser = subparsers.add_parser("derived",
	description = "Count proportion of calls for derived vs ancestral allele, using WSAF (default) or hard calls",
	parents = [parser])
anc_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
anc_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
anc_parser.set_defaults(func = count_derived, command = "derived")

## identify variants where non-ref allele is present in only specified subset of samples
priv_parser = subparsers.add_parser("private",
	description = "Annotate sites as 'private' in subset of samples.",
	parents = [parser])
priv_parser.add_argument("-f","--flag",
					default = "PRIV",
					help = "flag to add to INFO field for private sites [default: '%(default)s']")
priv_parser.add_argument("-s","--samples", nargs = "+",
					default = [],
					help = "one or more focal samples")
priv_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "focal samples are listed in this file [overrides `--samples`]")
priv_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
priv_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
priv_parser.add_argument("-c","--min-count", type = int,
					default = 1,
					help = "minimum supporting observations (reads or chromosomes) in focal sample(s) for allele to be called 'present' [default: %(default)d]")
priv_parser.set_defaults(func = flag_private, command = "private")

## calculate n-dimensional SFS
sfs_parser = subparsers.add_parser("sfs",
	description = "Estimate unfolded SFS by sampling from either read counts or hard calls.",
	parents = [parser])
sfs_parser.add_argument("-p","--pops", type = argparse.FileType("rU"),
					help = "file of (sample ID, population) tuples [default: all samples as one population]")
sfs_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
sfs_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
sfs_parser.add_argument("--ref-is-ancestral", action = "store_true",
					default = False,
					help = "assume reference allele is the ancestral one (FOR TESTING ONLY)")
sfs_parser.set_defaults(func = calc_sfs, command = "sfs")

## keep only every nth site
thin_parser = subparsers.add_parser("thin",
	description = "Reduce number of sites in VCF by emitting every nth site",
	parents = [parser])
thin_parser.add_argument("-Q", "--every", type = int,
						default = 10,
						help = "emit one site per this many visited [default: %(default)d]")
thin_parser.set_defaults(func = thin_sites, command = "thin")

## keep only every nth site
prune_parser = subparsers.add_parser("prune",
	description = "Reduce number of sites in VCF by emitting every nth site",
	parents = [parser])
prune_parser.add_argument("-w", "--window", type = int,
						default = 100,
						help = "window size in number of sites [default: %(default)d]")
prune_parser.add_argument("-s", "--step", type = int,
						default = 50,
						help = "offset between adjacent windows [default: %(default)d]")
prune_parser.add_argument("-r", "--r-squared", type = float,
						default = 0.1,
						help = "retain sites with r^2 below this threshhold [default: %(default)f]")
prune_parser.add_argument("-m", "--niter", type = int,
						default = 5,
						help = "perform this many rounds of pruning in each window before giving up [default: %(default)d]")
prune_parser.set_defaults(func = prune_sites_by_ld, command = "prune")

## parse command line
args = parser.parse_args()

## set up log trace
if args.quiet:
	logging.basicConfig(level = logging.CRITICAL)
elif args.verbose:
	logging.basicConfig(level = logging.DEBUG)
else:
	logging.basicConfig(level = logging.INFO)
logging.StreamHandler(stream = sys.stderr)
logger = logging.getLogger("vcfutils")
logger.debug("Command-line args, as parsed: {}".format(args))

## connect to VCF file
vcf = VCF(args.infile, gts012 = True)
nsites = args.nsites if args.nsites else -1
logger.info("Connecting to VCF file <{}>".format(args.infile))

## add a line to the header documenting what we did
cmd_line = " ".join(sys.argv[1:])
pwd = os.path.expandvars(os.getcwd())
timestamp = dt.now().strftime("%Y-%m-%d %H:%M:%S")
vcf.add_to_header("##vcfdo_command={}; pwd={}; timestamp={}".format(cmd_line, pwd, timestamp))

## do the requested thing
logger.info("Task: {}".format(args.command))
args.func(vcf = vcf, log = logging.getLogger(args.command), ns = nsites, **vars(args))
