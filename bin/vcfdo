#! /usr/bin/env python3
"""
vcfutils.py
Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.
"""

import os
import sys
import numpy as np
import itertools as it
import argparse
import logging

from re import split as resplit
from datetime import datetime as dt
from collections import defaultdict
from cyvcf2 import VCF
from allel import pca, randomized_pca
from pyfaidx import Fasta

def calc_wsaf(vcf, log, ns, **kwargs):

	vcf.add_info_to_header({ "ID": "PLAF", "Description": "Population-level ALT allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "PLMAF", "Description": "Population-level minor allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_info_to_header({ "ID": "UNW", "Description": "Population-level evidence for minor allele, as if all reads were pooled", "Number": 1, "Type": "Float" })
	vcf.add_format_to_header({ "ID": "WSAF", "Description": "Within-sample allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })
	vcf.add_format_to_header({ "ID": "WSMAF", "Description": "Within-sample MAJOR allele frequency weighted for mixed samples", "Number": 1, "Type": "Float" })

	do_filter = kwargs["set_filter"] is not None
	if do_filter:
		filter_at = min(1 - kwargs["set_filter"], kwargs["set_filter"])
		vcf.add_filter_to_header({ "ID": "PseudoHet", "Description": "WSAF out-of-bounds for a pure isolate" })
		vcf.add_format_to_header({ "ID": "FT", "Description": "Genotype-level filter", "Number": ".", "Type": "String" })

	log.info("Annotating VCF file: <{}>".format(kwargs["infile"]))
	sys.stdout.write(vcf.raw_header)

	## loop on VCF and add 'WSAF','PLAF','PLMAF' tags
	ii = 0
	nmulti = 0
	nnonpoly = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		## don't bother with multiallelic sites
		if len(site.ALT) > 1:
			nmulti += 1

		## don't bother with nonpolymorphic sites
		if len(site.ALT) < 1:
			nnonpoly += 1

		dp = site.format("AD")
		dp[ dp < 0 ] = 0
		## is depth recorded for >1 allele?
		if dp.shape[1] >= 2:
			# > 1 allele counted
			dp_ref = dp[:,0]
			dp_alt = np.sum(dp[:,1:], 1)
			dp_maj = np.amax(dp, 1)
			dp_tot = np.sum(dp, 1)

			## are there actually any reads for the non-ref alleles?
			dp_tot_all = np.sum(dp)
			dp_alt_all = np.sum(dp[:,1:])
			unw = dp_alt_all/dp_tot_all
			site.INFO["UNW"] = min(unw, 1-unw)

			wsmaf = dp_maj / dp_tot
			wsaf = np.ma.masked_invalid(dp_alt / (dp_ref + dp_alt), )
			plaf = np.average(wsaf)
			site.INFO["PLAF"] = plaf
			site.INFO["PLMAF"] = min(plaf, 1-plaf)
			# now undo masking of NaNs
			wsaf[ np.isnan(wsaf) ] = -1
			wsmaf[ np.isnan(wsmaf) ] = -1
			site.set_format("WSAF", wsaf)
			site.set_format("WSMAF", wsmaf)

		else:
			# only 1 allele counted
			site.INFO["PLAF"] = 0.0
			site.INFO["PLMAF"] = 0.0
			site.set_format("WSAF", np.tile(0.0, len(vcf.samples)))

		if do_filter:
			gt_filt = np.tile("PASS", len(vcf.samples))
			gt_filt[ np.logical_or(wsaf > args.set_filter, wsaf < (1-args.set_filter)) ] = "PseudoHet"
			if "GT" in site.FORMAT:
				old_filt = site.format("FT")
				for ii, flt in enumerate(old_filt):
					if flt != "PASS":
						gt_filt[ii] = flt + ";" + gt_filt[ii]
			site.set_format("FT", gt_filt)

		try:
			sys.stdout.write(str(site))
			#sys.stdout.flush()
		except BrokenPipeError as e:
			log.warning("Stopped because the pipe reading from this program has closed.".format(ii))
			break

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	log.info("Done.".format(ii))
	log.info("\t{: >12d} biallelic in called genotypes".format(ii-nmulti))
	log.info("\t{: >12d} non-polymorphic in called genotypes".format(nnonpoly))
	log.info("\t{: >12d} multiallelic".format(nmulti))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))


def calc_fws(vcf, log, ns, **kwargs):

	log.info("Processing VCF file: <{}>".format(kwargs["infile"]))

	## loop on VCF and add 'WSAF','PLAF','PLMAF' tags
	ii = 0
	nmulti = 0
	nnonpoly = 0
	nnowsaf = 0
	log.info("Processing VCF file ...")
	H_s = []
	H_p = []

	for site in vcf:

		ii += 1

		## don't bother with multiallelic sites
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		## don't bother with nonpolymorphic sites
		if len(site.ALT) < 1:
			nnonpoly += 1
			continue

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		#if (not "WSAF" in site.FORMAT) or (not "PLMAF" in site.INFO):
		#	nnowsaf += 1
		#	continue

		wsaf = np.ma.masked_values(site.format("WSAF"), -1)
		plmaf = site.INFO["PLMAF"]
		H_px = 1 - (plmaf**2 + (1 - plmaf)**2)
		H_sx = 1 - (np.power(wsaf, 2) + np.power((1 - wsaf), 2))
		H_p.append(H_px)
		H_s.append(H_sx)

		if ii >= ns and ns > -1:
			break

	log.info("Done reading VCF.".format(ii))
	log.info("\t{: >12d} biallelic in called genotypes".format(ii-nmulti))
	log.info("\t{: >12d} non-polymorphic in called genotypes".format(nnonpoly))
	log.info("\t{: >12d} multiallelic".format(nmulti))
	log.info("\t{: >12d} without WSAF,PLMAF annotations".format(nnowsaf))
	log.info("\t------------")
	log.info("\t{: >12d} total sites\n".format(ii))

	log.info("Calculating F_ws ...")
	## matrices of H_px and H_sx
	H_p = np.array(H_p, dtype = np.float).reshape(len(H_p), 1)
	H_s = np.ma.asanyarray(H_s)[:,:,0]
	#print(np.sum(H_s.mask))
	#print(H_s.shape)

	## do no-intercept regression of H_sx on H_px
	for ii in range(0, len(vcf.samples)):
		H_sx = H_s[:,ii]
		X = H_sx[ ~H_sx.mask ]
		X = X.reshape(len(X), 1)
		y = H_p[ ~H_sx.mask ]
		#print(X.shape, y.shape)
		beta, sse, rank, sv = np.linalg.lstsq(y, X)
		sigma2 = np.sum((y - np.dot(X, beta))**2) / (X.shape[0] - 1)
		C = sigma2 * np.linalg.pinv(np.dot(X.T, X)) # covariance matrix
		se = np.sqrt(np.diag(C)) # standard error
		F_ws = 1 - beta
		print(vcf.samples[ii], F_ws[0,0], se[0], np.sum(~H_sx.mask), sep = "\t")

	log.info("Done.\n")


def pca_wsaf(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## decide which samples to use
	if args.samples_file:
		keep_samples = []
		for line in args.samples_file:
			if line.startswith("#"):
				continue
			else:
				keep_samples.append( line.strip().split().pop(0) )
		log.info("Read {} samples from file <{}>".format(len(keep_samples), args.samples_file.name))
	elif args.samples is not None and len(args.samples):
		keep_samples = args.samples
		log.info("Read {} samples from command line".format(len(keep_samples)))
	else:
		keep_samples = None

	if keep_samples is not None:
		keep_samples = set(vcf.samples) & set(keep_samples)
		keep_samples = list(keep_samples)
		nsamples = len(keep_samples)
		log.info("Retained {} distinct samples that were verified in VCF header.".format(nsamples))
	else:
		keep_samples = list(vcf.samples)
		nsamples = len(vcf.samples)
		log.info("Retained all {} samples in VCF.".format(nsamples))

	CHUNK_SIZE = args.chunk_size
	ii = 0
	nkept = 0
	W = np.zeros((CHUNK_SIZE, nsamples), dtype = np.float)
	log.info("Reading genotype data in chunks of size {} ...".format(CHUNK_SIZE))

	sample_idx = [ vcf.samples.index(_) for _ in keep_samples ]
	for site in vcf:

		ii += 1
		if site.INFO["PLMAF"] < args.min_plmaf:
			continue

		nkept += 1
		if nkept >= W.shape[0]:
			log.debug("Increasing array allocation ... ")
			W = np.append( W, np.zeros((CHUNK_SIZE, nsamples), dtype = np.float), axis = 0 )

		## get WSAFs from FORMAT field
		w = site.format("WSAF")
		w = w[sample_idx,0]

		## impute PLAF for missing values
		plaf = np.average(w[ w != -1 ])
		ww = w[:]
		ww[ ww == -1 ] = plaf
		W[nkept,:] = ww

		if not ii % 1000:
			log.info("\t... processed {} sites [kept {}]...".format(ii, nkept))

		if ii >= ns and ns > -1:
			break

	## remove low-variance sites that make bad things happen in SVD
	W = W[:nkept,:]
	sigma = np.var(W, 1)
	W = W[ (sigma > 1.0e-6),:]
	log.info("Loaded genotypes at {} sites with PLMAF >= {}".format(nkept, args.min_plmaf))
	log.info("Retained {} sites with non-negligible variance for PCA.".format(W.shape[0]))

	## do the PCA using scikit-allel tools
	log.info("Performing PCA ...")
	pca_fn = pca if not args.randomized else randomized_pca
	coords, model = pca_fn(W, n_components = args.ncomponents, scaler = "patterson")
	log.info("Done with PCA.")

	## write rotations and normalized eigenvalues
	log.info("Writing results to <{0}.pca> and <{0}.pca.ev> (sample coordinates and variance explained, respectively.)".format(args.out))
	with open(args.out + ".pca", "w") as pcs_file:
		for ii in range(0, coords.shape[0]):
			print(vcf.samples[ii], *coords[ii,:], sep = "\t", file = pcs_file)
	with open(args.out + ".pca.ev", "w") as evs_file:
		print("\n".join([ str(_) for _ in model.explained_variance_ratio_ ]), file = evs_file)

	## all done
	log.info("Done.\n")


def calc_weighted_dist(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)

	CHUNK_SIZE = args.chunk_size
	def give_chunk():
		return np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.float)

	wsaf = give_chunk()
	wsaf_imputed = give_chunk()

	## loop on VCF and add take sample depths
	ii = 0
	logger.info("Reading genotype data in chunks of size {} ...".format(CHUNK_SIZE))
	for site in vcf:

		ii += 1
		if ii >= wsaf.shape[0]:
			log.debug("Increasing array allocation ... ")
			wsaf = np.append( wsaf, give_chunk(), axis = 0 )
			wsaf_imputed = np.append( wsaf_imputed, give_chunk(), axis = 0 )

		## get WSAFs from FORMAT field
		this_wsaf = site.format("WSAF")
		wsaf[ii,:] = this_wsaf[:,0]

		## impute PLAF for missing values
		plaf = np.average(this_wsaf[ this_wsaf != -1 ])
		ww = this_wsaf[:,0]
		ww[ ww == -1 ] = plaf
		wsaf_imputed[ii,:] = ww

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	wsaf = wsaf[ :ii, : ]
	wsaf_imputed = wsaf_imputed[ :ii, : ]
	log.info("Dimensions of full genotype matrix: {}".format(wsaf.shape))
	log.info("Calculating weighting factors following Amato et al (PMC4786412) ...")
	r_sq = np.corrcoef(wsaf_imputed) ** 2
	r_sq[ np.isnan(r_sq) ] = 0
	r_sq -= np.eye(r_sq.shape[0])
	r_sq[ r_sq < kwargs["rsq_cutoff"] ] = 0
	w_i = 1 / (1 + np.sum(r_sq, 1))

	log.info("Calculating pairwise distances using WSAFs ...")
	combos = list( it.combinations(range(0, len(vcf.samples)), 2) )
	log.info("\t[{} combinations]".format(len(combos)))
	#D = np.zeros( (len(vcf.samples), len(vcf.samples)), dtype = np.float )
	ii = 0
	for sa,sb in combos:
		ii += 1
		fa = np.ma.masked_values(wsaf[:,sa], -1)
		fb = np.ma.masked_values(wsaf[:,sb], -1)
		d_i = (fa*(1-fb) + (1-fa)*fb)*w_i
		d = np.average(d_i)
		#D[ sa,sb ] = np.average(d*w_i)
		nz = np.sum(~d_i.mask)
		if not ii % 100:
			log.info("\t... {} pairs done ...".format(ii))
		print(vcf.samples[sa], vcf.samples[sb], nz, d, sep = "\t")

	log.info("Done.\n")

def summarise_dp(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)

	CHUNK_SIZE = args.chunk_size
	sample_dp = np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.int)

	## loop on VCF and add take sample depths
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1
		if ii >= sample_dp.shape[0]:
			log.debug("Increasing array allocation ... ")
			sample_dp = np.append( sample_dp, np.zeros((CHUNK_SIZE,len(vcf.samples)), dtype = np.int), axis = 0 )

		this_dp = site.format("DP").T[0:]
		this_dp[ this_dp < 0 ] = 0
		sample_dp[ii,:] = this_dp

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	mu = np.mean(sample_dp[:ii,:], 0)
	sigma = np.std(sample_dp[:ii,:], 0)
	med = np.percentile(sample_dp[:ii,:], 50, 0)

	for iid, _mu, _med, _sigma in zip(vcf.samples, mu, med, sigma):
		print(iid, "{:.1f}".format(_mu), "{:.1f}".format(_med), "{:.3f}".format(_sigma), sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))


def summarise_flt(vcf, log, ns, **kwargs):

	## loop on VCF and add take sample depths
	ii = 0
	what_mode = None
	if args.samples:
		counter = (
			{ iid: defaultdict(int) for iid in vcf.samples },
			{ iid: defaultdict(int) for iid in vcf.samples }
		)
		what_mode = "SAMPLES"
	elif args.sites:
		counter = defaultdict(int)
		what_mode = "SITES"
	else:
		log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	log.info("Talling filters by {}".format(what_mode))
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		site_filt = "PASS" if site.FILTER is None else site.FILTER
		if what_mode == "SITES":
			counter[site_filt] += 1
		elif what_mode == "SAMPLES":
			jj = 0 if site_filt == "PASS" else 1
			if "FT" in site.FORMAT:
				for iid, geno_filt in zip(vcf.samples, site.format("FT")):
					counter[jj][iid][geno_filt] += 1
			else:
				for iid in vcf.samples:
					counter[jj][iid]["PASS"] += 1

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	#print(counter)
	if what_mode == "SITES":

		for filt,count in counter.items():
			print(filt, count, sep = "\t")

	elif what_mode == "SAMPLES":

		for iid in vcf.samples:
			for jj in (0,1):
				flag = "PASS" if ii else "FAIL"
				for filt,count in counter[jj][iid].items():
					print(iid, flag, filt, count, sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))


def calc_missingness(vcf, log, ns, **kwargs):

	## loop on VCF and count missing calls
	ii = 0
	what_mode = None
	if args.samples:
		counts = np.zeros(len(vcf.samples), dtype = np.int)
		what_mode = "SAMPLES"
	elif args.sites:
		what_mode = "SITES"
	else:
		log.error("Must specify either --sites or --samples.")
		sys.exit(1)

	log.info("Talling missingness by {}".format(what_mode))
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if what_mode == "SITES":
			nmiss = np.sum(site.gt_types == 3)
			ntot = len(vcf.samples)
			print(site.CHROM, site.POS, site.ID, site.REF, ",".join(site.ALT), nmiss, nmiss/ntot, sep = "\t")
		elif what_mode == "SAMPLES":
			counts += (site.gt_types == 3)
		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	#print(counter)
	if what_mode == "SITES":
		pass
	elif what_mode == "SAMPLES":
		for iid,nmiss in zip(vcf.samples, counts):
			print(iid, nmiss, nmiss/ii, sep = "\t")

	log.info("Done.".format(ii))
	log.info("\t{: >12d} total sites".format(ii))

def polarize_anc_vs_der(vcf, log, ns, **kwargs):

	## connect to ancestral genome file
	genome = Fasta(args.fasta)
	log.info("Ancestral genome: <{}>".format(args.fasta))

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": "AA", "Description": "Ancestral allele inferred from outgroup", "Number": 1, "Type": "String" })
	sys.stdout.write(vcf.raw_header)

	def define_anc_allele(alleles, og):
		ref = alleles[0].upper()
		alt = [ x.upper() for x in alleles[1:] ]
		new_alleles = [ref] + alt
		og = og.upper()
		anc = "X"*len(ref)
		for ii,a in enumerate(new_alleles):
			if a == og:
				anc = alleles[ii]
				break
		return anc

	## loop on VCF and add 'AA' tag
	nanc = 0
	nder = 0
	nmissing = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		start = site.POS - 1
		end = start + len(site.REF)
		alleles = [site.REF] + site.ALT
		og = str(genome[site.CHROM][start:end])
		anc = define_anc_allele(alleles, og)
		#print(og, alleles, anc, file = sys.stderr)

		if anc == "X":
			nmissing += 1
		elif anc == alleles[0]:
			nder += 1
		elif anc in alleles[1:]:
			nanc += 1

		site.INFO["AA"] = anc
		sys.stdout.write(str(site))

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		if ii >= ns and ns > -1:
			break

	log.info("Done.".format(ii))
	log.info("\t{: >12d} had undefined ancestral allele".format(nmissing))
	log.info("\t{: >12d} have REF = ancestral".format(nder))
	log.info("\t{: >12d} have REF = derived".format(nanc))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))


def count_derived(vcf, log, ns, **kwargs):

	args = argparse.Namespace(**kwargs)
	if not (args.reads or args.calls) or (args.reads and args.calls):
		log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	log.info("Counting derived alleles at level of {}".format(how_count))

	## initialize counters
	what_type = np.int if args.calls else np.float
	counts = np.zeros(len(vcf.samples), dtype = what_type)
	nsites = np.zeros(len(vcf.samples), dtype = np.int)

	## loop on VCF
	ploidy = None
	nambig = 0
	npol = 0
	nmulti = 0
	nuncountable = 0
	nused = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if ii > ns and ns > -1:
			break

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		## absent or bogus ancestral allele?
		anc = "X"
		try:
			anc = site.INFO["AA"]
			#log.info("Ancestral allele is '{}'".format(anc))
		except KeyError as e:
			nambig += 1
			continue

		if anc == "X":
			nambig += 1
			continue

		npol += 1

		## not biallelic?
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		## sniff ploidy
		if ploidy is None:
			pieces = resplit(r"[/|]", site.gt_bases[0])
			ploidy = len(pieces)
			log.info("Detected ploidy = {} in this VCF".format(ploidy))

		## biallelic, has non-bogus ancestral allele
		if args.reads:
			if not "WSAF" in site.FORMAT:
				nuncountable += 1
				continue
			else:
				gt = site.format("WSAF")[:,0]
				if not site.REF == anc:
					gt = 1 - gt
				nsites[ gt > -1 ] += 1
				counts[ gt != -1 ] += gt[ gt != -1 ]
				nused += 1
		else:
			gt = site.gt_types
			nsites[ gt < 3 ] += 1
			counts[ gt < 3 ] += gt[ gt < 3 ]
			nused += 1

	if args.calls:
		counts = counts/ploidy
	for jj,iid in enumerate(vcf.samples):
		print(iid, counts[jj], nsites[jj], counts[jj]/nsites[jj], ploidy, sep = "\t")

	log.info("Done.")
	log.info("\t{: >12d} had usable data".format(nused))
	log.info("\t{: >12d} had undefined ancestral allele".format(nambig))
	log.info("\t{: >12d} had >2 alleles".format(nmulti))
	if args.reads:
		log.info("\t{: >12d} had no WSAF annotation".format(nuncountable))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))

def flag_private(vcf, log, ns, **kwargs):

	## load args into a namespace, so I don't have to recode so much stuff
	args = argparse.Namespace(**kwargs)

	## identify private alleles by hard calls or read counts?
	if not (args.reads or args.calls) or (args.reads and args.calls):
		log.error("Must specify either `--calls` OR `--reads`, and not both.")
		sys.exit(1)

	how_count = "READS" if args.reads else "GENOTYPE CALLS"
	what_count = "READS" if args.reads else "CHROMOSOMES"
	log.info("Defining private alleles at level of {}".format(how_count))
	log.info("Cutoff for allele presence: {} supporting {}".format(args.min_count, what_count))

	## decide which samples to use
	if args.samples_file:
		keep_samples = []
		for line in args.samples_file:
			if line.startswith("#"):
				continue
			else:
				keep_samples.append( line.strip().split().pop(0) )
		log.info("Read {} samples from file <{}>".format(len(keep_samples), args.samples_file.name))
	elif len(args.samples):
		keep_samples = []
		for iid in args.samples:
			keep_samples.extend(resplit("[,;|]", iid))
		log.info("Read {} samples from command line".format(len(keep_samples)))
	else:
		log.error("Must specify at least one focal sample.")
		sys.exit(1)

	keep_samples = set(vcf.samples) & set(keep_samples)
	keep_samples = list(keep_samples)
	nsamples = len(keep_samples)
	log.info("Retained {} distinct samples that were verified in VCF header.".format(nsamples))

	## get sample positions in input VCF
	focal_idx = [ vcf.samples.index(_) for _ in keep_samples ]
	nonfocal_idx = [ _ for _,iid in enumerate(vcf.samples) if not iid in keep_samples ]

	def is_private(counts):
		if args.reads:
			counts[ counts < 0 ] = 0
		else:
			counts[ counts == 3 ] = 0
		ac_in = np.sum(counts[focal_idx])
		ac_out = np.sum(counts[nonfocal_idx])
		return ac_in >= args.min_count and ac_out < args.min_count

	## modify outgoing VCF header
	vcf.add_info_to_header({ "ID": args.flag, "Description": "Private allele in specified subset", "Type": "Flag", "Number": "1"})
	sys.stdout.write(vcf.raw_header)

	nused = 0
	nnodp = 0
	nmulti = 0
	npriv = 0
	ii = 0
	log.info("Processing VCF file ...")
	for site in vcf:

		ii += 1

		if ii > ns and ns > -1:
			break

		if not ii % 1000:
			log.info("\t... processed {} sites ...".format(ii))

		## not biallelic?
		if len(site.ALT) > 1:
			nmulti += 1
			continue

		if args.reads:
			if not "AD" in site.FORMAT:
				nnodp += 1
				continue
			counts = site.format("AD")[:,1]
		else:
			counts = site.gt_types[:]

		if is_private(counts):
			npriv += 1
			site.INFO[args.flag] = True

		sys.stdout.write(str(site))

	log.info("Done.")
	log.info("\t{: >12d} had usable data".format(nused))
	log.info("\t[{: >12d} flagged private to {} samples]".format(npriv, nsamples))
	if args.reads:
		log.info("\t{: >12d} had no AD annotation".format(nnodp))
	log.info("\t{: >12d} had >2 alleles".format(nmulti))
	log.info("\t------------")
	log.info("\t{: >12d} total sites".format(ii))


## root-level argument parser
def root_help_message(name = None):
	return r"""
	vcfdo <command> [options]

	Commands:

	  --- Basic statistics
	      missing           rates of missing calls by site or by sample
	      depthsummary      quantiles of read depth (at called sites) per sample
	      filtersummary     tallies of filter status by site or by sample

	  --- Ancestral alleles
	      polarize          define ancestral alleles based on outgroup sequence
	      derived           count derived alleles per sample, using either read counts or hard calls

	  --- Allele frequencies
	      wsaf              calculate within-sample allele frequency (WSAF) and related quantities from read counts
	      fws               estimate pseudo-inbreeding coefficient F_ws ("within-sample relatedness")
	      private           annotate sites where non-ref allele is only found in certain subset of samples

	  --- Relatedness/ordination
	      dist              calculate LD-weighted pairwise distances from within-sample allele frequencies
	      pca               perform PCA using within-sample allele frequencies instead of hard calls

	Notes:
	-- All commands that take VCF as input or output work on pipes. For basic filtering operations,
	it is recommended to use `bcftools` and pipe to this tool.
	-- Input can be in any format that `htslib` can handle -- VCF, bgzipped VCF, BCF, etc.
	-- Commands which modify the VCF will add an entry to the header recording the command line used,
	the current working directory and a timestamp.
	"""

parser = argparse.ArgumentParser(
	description = "Some utilities for annotating, summarising and filtering VCF files with an eye towards Plasmodium spp.",
	usage = root_help_message(),
	add_help = False)
subparsers = parser.add_subparsers()
parser.add_argument("-i","--infile",
					default = "/dev/stdin",
					help = "VCF file, possibly gzipped [default: stdin]")
parser.add_argument("-n","--nsites", type = int,
					help = "stop after processing this many sites [default: no limit]")
parser.add_argument("--chunk-size", type = int,
					default = 2000,
					help = "when loading genotypes into memory, process this many sites per chunk [default: %(default)d]")
parser.add_argument("-q","--quiet", action = "store_true",
					help = "suppress logging messages")

## calculate WSAF, PLAF, PLMAF
wsaf_parser = subparsers.add_parser("wsaf",
	description = "Calculate within-sample allele frequencies (WSAF) from read counts, and their population-level equivalents.",
	parents = [parser])
wsaf_parser.add_argument("--set-filter", type = float,
						default = None,
						help = "apply genotype-level filter for samples with WSAF outside specified bounds [default: don't do this]")
wsaf_parser.set_defaults(func = calc_wsaf, command = "wsaf")

## calculate F_ws from WSAF
wsaf_parser = subparsers.add_parser("fws",
	description = "Calculate within-sample allele 'inbreeding'-like coefficient F_ws, using precomputed WSAFs.",
	parents = [parser])
wsaf_parser.set_defaults(func = calc_fws, command = "fws")

## summarise depth per sample per called site
dp_parser = subparsers.add_parser("depthsummary",
	description = "Summarise depth (FORMAT:DP) per site per sample, splitting by filter status of sites.",
	parents = [parser])
dp_parser.set_defaults(func = summarise_dp, command = "depthsummary")

## summarise filters at site or individual level
flt_parser = subparsers.add_parser("filtersummary",
	description = "Tally filter status of sites or samples.",
	parents = [parser])
flt_parser.add_argument("--sites", action = "store_true",
						help = "tally filter status of sites")
flt_parser.add_argument("--samples", action = "store_true",
						help = "tally filter status of per-sample genotypes")
flt_parser.set_defaults(func = summarise_flt, command = "filtersummary")

## summarise filters at site or individual level
miss_parser = subparsers.add_parser("missing",
	description = "Tally missing calls per site or per sample.",
	parents = [parser])
miss_parser.add_argument("--sites", action = "store_true",
						help = "tally missingness per site")
miss_parser.add_argument("--samples", action = "store_true",
						help = "tally missingness per sample, across all sites")
miss_parser.set_defaults(func = calc_missingness, command = "missing")

## do PCA on WSAFs
pca_parser = subparsers.add_parser("pca",
	description = "Perform PCA on with-sample allele frequencies instead of genotype calls.",
	parents = [parser])
pca_parser.add_argument("-o","--out",
					default = "wsaf_pca",
					help = "prefix for a pair of output files, *.pca and *.pca.ev [default: %(default)s]")
pca_parser.add_argument("-s","--samples", nargs = "+",
					help = "restrict analysis to these samples, whitespace-separated [default: use all samples]")
pca_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "restrict analysis to samples in this file, one per line [default: use all samples]")
pca_parser.add_argument("-f","--min-plmaf", type = float,
					default = 0.0,
					help = "limit to sites with PMLAF > {min_plmaf} [default: %(default)f]")
pca_parser.add_argument("-K","--ncomponents", type = int,
					default = 10,
					help = "calculate the first K PCs if using randomized SVD algorithm [default: %(default)d]")
pca_parser.add_argument("--randomized", action = "store_true",
					help = "use fast randomized SVD algorithm")
pca_parser.set_defaults(func = pca_wsaf, command = "pca")

dist_parser = subparsers.add_parser("dist",
	description = "Calculate LD-weighted genetic distances from within-sample allele frequencies according to method of Amato et al. (PMC4786412).",
	parents = [parser])
dist_parser.add_argument("-r","--rsq-cutoff", type = float,
					default = 0.10,
					help = "ignore site pairs with r^2 < cutoff when assigning weights [default: %(default)f]")
dist_parser.set_defaults(func = calc_weighted_dist, command = "dist")

anc_parser = subparsers.add_parser("polarize",
	description = "Polarize alleles as ancestral vs derived, based on outgroup sequence",
	parents = [parser])
anc_parser.add_argument("-f","--fasta",
					required = True,
					help = "ancestral genome fasta file")
anc_parser.set_defaults(func = polarize_anc_vs_der, command = "polarize")

anc_parser = subparsers.add_parser("derived",
	description = "Count proportion of calls for derived vs ancestral allele, using WSAF (default) or hard calls",
	parents = [parser])
anc_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
anc_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
anc_parser.set_defaults(func = count_derived, command = "derived")

priv_parser = subparsers.add_parser("private",
	description = "Annotate sites as 'private' in subset of samples.",
	parents = [parser])
priv_parser.add_argument("-f","--flag",
					default = "PRIV",
					help = "flag to add to INFO field for private sites [default: '%(default)s']")
priv_parser.add_argument("-s","--samples", nargs = "+",
					default = [],
					help = "one or more focal samples")
priv_parser.add_argument("-S","--samples-file", type = argparse.FileType("rU"),
					help = "focal samples are listed in this file [overrides `--samples`]")
priv_parser.add_argument("--reads", action = "store_true",
					default = False,
					help = "count supporting reads (NB: site-wise, not haplotype-aware) [default: %(default)s]")
priv_parser.add_argument("--calls", action = "store_true",
					default = False,
					help = "count hard genotype calls rather than supporting reads [default: %(default)s]")
priv_parser.add_argument("-c","--min-count", type = int,
					default = 1,
					help = "minimum supporting observations (reads or chromosomes) in focal sample(s) for allele to be called 'present' [default: %(default)d]")
priv_parser.set_defaults(func = flag_private, command = "private")

## parse command line
args = parser.parse_args()

## set up log trace
if args.quiet:
	logging.basicConfig(level = logging.CRITICAL)
else:
	logging.basicConfig(level = logging.INFO)
logging.StreamHandler(stream = sys.stderr)
logger = logging.getLogger("vcfutils")
logger.debug("Command-line args, as parsed: {}".format(args))

## connect to VCF file
vcf = VCF(args.infile, gts012 = True)
nsites = args.nsites if args.nsites else -1
logger.info("Connecting to VCF file <{}>".format(args.infile))

## add a line to the header documenting what we did
cmd_line = " ".join(sys.argv[1:])
pwd = os.path.expandvars(os.getcwd())
timestamp = dt.now().strftime("%Y-%m-%d %H:%M:%S")
vcf.add_to_header("##vcfutils_command={}; pwd={}; timestamp={}".format(cmd_line, pwd, timestamp))

## do the requested thing
logger.info("Task: {}".format(args.command))
args.func(vcf = vcf, log = logging.getLogger(args.command), ns = nsites, **vars(args))
